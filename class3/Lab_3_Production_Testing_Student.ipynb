{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Lab 3: Production-Grade Testing Checklist\n",
    "\n",
    "**Class 3 — Non-Functional Testing & AI Security**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will produce **four deliverables** that together form a production testing checklist for the ADAS model API:\n",
    "\n",
    "| Deliverable | Description |\n",
    "|-------------|-------------|\n",
    "| **1. Performance Benchmark Report** | DataFrame + chart: latency, throughput, memory by batch size |\n",
    "| **2. Security Checklist** | 8-item security assessment with pass/fail status |\n",
    "| **3. Automated Test Scripts** | Three pytest test classes (API response, accuracy, latency) |\n",
    "| **4. Quantized Model Comparison** | Side-by-side comparison: FP32 vs INT8 (size, speed, accuracy) |\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all cells marked with `# TODO:`\n",
    "2. Run the entire notebook from top to bottom after completing each section\n",
    "3. Review the **Final Checklist** at the end and fill in the pass/fail column\n",
    "4. Save the notebook — it IS your submission\n",
    "\n",
    "**Reference materials:**\n",
    "- Part 1: `Part_1_Performance_Benchmarking.ipynb`\n",
    "- Part 2: `Part_2_Security_Hardening.ipynb`\n",
    "- Part 3: `Part_3_Automated_Testing.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip**: Don't just copy-paste from the Part notebooks. Understand each piece before you write it."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "# ── Setup (provided — do not modify) ────────────────────────────────────────\nimport os, io, sys, time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom pathlib import Path\nfrom PIL import Image\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Dict\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\nfrom fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Request\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom fastapi.testclient import TestClient\nfrom pydantic import BaseModel\nfrom jose import JWTError, jwt\nimport bcrypt as _bcrypt\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCLASS_NAMES = ['animal', 'name_board', 'other_vehicle', 'pedestrian',\n               'pothole', 'road_sign', 'speed_breaker']\nNUM_CLASSES = len(CLASS_NAMES)\nDATASET_PATH = r'C:\\Users\\Lucifer\\python_workspace\\BITS\\AI_Quality_Engineering\\dataset'\nTEST_PATH = os.path.join(DATASET_PATH, 'test')\n\n# Model (random weights for demo)\nclass ADASModel(nn.Module):\n    def __init__(self, num_classes=7):\n        super().__init__()\n        self.resnet = resnet18(weights=None)\n        self.resnet.fc = nn.Linear(512, num_classes)\n    def forward(self, x): return self.resnet(x)\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)), transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\nmodel = ADASModel(NUM_CLASSES).to(DEVICE)\nmodel.eval()\n\ndef create_test_image(size=(100, 100), color='red', fmt='PNG'):\n    img = Image.new('RGB', size, color=color)\n    buf = io.BytesIO()\n    img.save(buf, format=fmt)\n    buf.seek(0)\n    return buf, f'test.{fmt.lower()}'\n\nprint(f'Setup complete. Device: {DEVICE}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": [
    "---\n",
    "## Deliverable 1: Performance Benchmark Report\n",
    "\n",
    "Complete the benchmarking functions and produce a DataFrame + chart."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Deliverable 1: Performance Benchmarking ──────────────────────────────────\n",
    "\n",
    "def measure_latency(model, batch_tensor, n_warmup=5, n_runs=50):\n",
    "    \"\"\"\n",
    "    Measure inference latency in milliseconds.\n",
    "    Returns dict with p50, p95, p99, mean latency.\n",
    "    \"\"\"\n",
    "    # TODO: Implement warmup loop (n_warmup runs, no timing)\n",
    "    # Hint: use torch.no_grad() and call model(batch_tensor)\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "    # TODO: Implement timed measurement loop (n_runs runs, record latencies)\n",
    "    # Hint: use time.perf_counter() before and after each model call\n",
    "    # Convert to milliseconds (* 1000)\n",
    "    # YOUR CODE HERE\n",
    "    latencies = []\n",
    "\n",
    "    # TODO: Return dict with p50, p95, p99, mean in ms\n",
    "    # Hint: use np.percentile(latencies, 50) etc.\n",
    "    return {\n",
    "        'p50_ms':  0.0,  # TODO: replace with np.percentile\n",
    "        'p95_ms':  0.0,  # TODO: replace\n",
    "        'p99_ms':  0.0,  # TODO: replace\n",
    "        'mean_ms': 0.0,  # TODO: replace\n",
    "    }\n",
    "\n",
    "\n",
    "def measure_throughput(model, batch_size, n_runs=20):\n",
    "    \"\"\"\n",
    "    Measure throughput in images/second.\n",
    "    Formula: batch_size / mean_time_per_batch_in_seconds\n",
    "    \"\"\"\n",
    "    dummy = torch.randn(batch_size, 3, 128, 128).to(DEVICE)\n",
    "    # TODO: Time n_runs model calls, compute mean time, return throughput\n",
    "    # YOUR CODE HERE\n",
    "    return 0.0  # TODO: return batch_size / mean_time\n",
    "\n",
    "\n",
    "def get_model_size_mb(model, path='_tmp.pth'):\n",
    "    \"\"\"Save model weights to temp file, measure size, delete.\"\"\"\n",
    "    # TODO: Save model.state_dict() to path, measure file size, remove file, return MB\n",
    "    # YOUR CODE HERE\n",
    "    return 0.0  # TODO: return size in MB\n",
    "\n",
    "\n",
    "# TODO: Run the benchmark loop for batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "# For each batch size, measure: latency (p50/p95/p99), throughput, memory delta\n",
    "# Store results in a list of dicts, then convert to pd.DataFrame\n",
    "# YOUR CODE HERE\n",
    "BATCH_SIZES = [1, 2, 4, 8, 16, 32]\n",
    "benchmark_results = []\n",
    "\n",
    "# --- your benchmark loop here ---\n",
    "\n",
    "df_bench = pd.DataFrame(benchmark_results)  # should have columns: batch_size, p50_ms, p95_ms, p99_ms, throughput_img_per_s\n",
    "print('Benchmark DataFrame:')\n",
    "print(df_bench)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Create a 2-panel visualization:\n",
    "#   Panel 1: Latency (p50, p95, p99) vs batch size — line chart\n",
    "#   Panel 2: Throughput (img/s) vs batch size — line chart\n",
    "# Save as 'lab3_benchmark.png'\n",
    "# YOUR CODE HERE\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Lab 3 — Benchmark Report', fontsize=14)\n",
    "\n",
    "# TODO: Plot latency chart in axes[0]\n",
    "# TODO: Plot throughput chart in axes[1]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lab3_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: lab3_benchmark.png')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Quantized Model Comparison (Deliverable 4 prereq) ────────────────────────\n",
    "\n",
    "# TODO: Apply dynamic quantization to the model\n",
    "# Hint: torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "# YOUR CODE HERE\n",
    "model_quantized = None  # TODO: replace\n",
    "\n",
    "# TODO: Compare sizes\n",
    "size_original  = get_model_size_mb(model)\n",
    "size_quantized = get_model_size_mb(model_quantized) if model_quantized else 0.0\n",
    "\n",
    "# TODO: Compare latency (use batch size 1 on CPU)\n",
    "model_cpu = model.to('cpu')\n",
    "dummy_cpu = torch.randn(1, 3, 128, 128)\n",
    "lat_orig  = measure_latency(model_cpu, dummy_cpu)\n",
    "lat_quant = {'p50_ms': 0.0, 'p95_ms': 0.0, 'p99_ms': 0.0, 'mean_ms': 0.0}  # TODO: measure quantized\n",
    "\n",
    "# TODO: Evaluate accuracy of both models on test set\n",
    "test_dataset = ImageFolder(TEST_PATH, transform=transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "acc_original  = 0.0  # TODO: evaluate model accuracy\n",
    "acc_quantized = 0.0  # TODO: evaluate model_quantized accuracy\n",
    "\n",
    "# TODO: Print comparison table\n",
    "print('=== Quantization Comparison ===')\n",
    "print(f'Original   — Size: {size_original:.2f}MB, Acc: {acc_original:.2f}%, p50: {lat_orig[\"p50_ms\"]:.2f}ms')\n",
    "print(f'Quantized  — Size: {size_quantized:.2f}MB, Acc: {acc_quantized:.2f}%, p50: {lat_quant[\"p50_ms\"]:.2f}ms')\n",
    "model = model.to(DEVICE)  # restore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": [
    "---\n",
    "## Deliverable 2: Security Checklist\n",
    "\n",
    "Build and test the secure API, then complete the security assessment."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "# ── Deliverable 2: Security Checklist ────────────────────────────────────────\n\n# --- JWT Setup ---\nSECRET_KEY = 'lab3-demo-secret-key'\nALGORITHM  = 'HS256'\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n\nFAKE_USERS_DB = {\n    'labuser': {'username': 'labuser',\n                'hashed_password': _bcrypt.hashpw(b'labpass', _bcrypt.gensalt()),\n                'disabled': False}\n}\n\n# TODO: Implement create_access_token(data, expires_delta)\n# Hint: copy pattern from Part 2 or api_secure.py\ndef create_access_token(data: dict, expires_delta=None) -> str:\n    # YOUR CODE HERE\n    pass  # TODO\n\n\n# TODO: Implement get_current_user(token) as a FastAPI dependency\n# It should raise HTTP 401 if the token is invalid or expired\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    # YOUR CODE HERE\n    pass  # TODO\n\n\n# --- Rate Limiter ---\n# TODO: Implement RateLimiter class with is_allowed(client_id) method\n# Use a sliding window (defaultdict of timestamp lists)\nclass RateLimiter:\n    def __init__(self, max_requests=5, window_seconds=60):\n        self.max_requests   = max_requests\n        self.window_seconds = window_seconds\n        self._requests = defaultdict(list)\n\n    def is_allowed(self, client_id: str) -> bool:\n        # TODO: prune old timestamps, check limit, record new timestamp\n        # YOUR CODE HERE\n        return True  # TODO: replace with real logic\n\n    def reset(self):\n        self._requests.clear()\n\n\nrate_limiter = RateLimiter(max_requests=3, window_seconds=60)\n\n# --- Build the secure FastAPI app ---\nclass Token(BaseModel):\n    access_token: str; token_type: str\n\nclass PredResponse(BaseModel):\n    prediction: str; confidence: float; model_version: str; latency_ms: float\n\nsecure_app = FastAPI(title='Lab3 Secure API')\n\n@secure_app.post('/token', response_model=Token)\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = FAKE_USERS_DB.get(form_data.username)\n    if not user or not _bcrypt.checkpw(form_data.password.encode('utf-8'), user['hashed_password']):\n        raise HTTPException(status_code=401, detail='Invalid credentials')\n    token = create_access_token({'sub': user['username']}, timedelta(minutes=30))\n    return {'access_token': token, 'token_type': 'bearer'}\n\n@secure_app.get('/health')\nasync def health(): return {'status': 'healthy'}\n\n# TODO: Implement the /predict endpoint with:\n#   - JWT authentication (Depends(get_current_user))\n#   - Rate limiting (check rate_limiter.is_allowed)\n#   - File validation (extension, corruption, size)\n#   - Model inference\n@secure_app.post('/predict', response_model=PredResponse)\nasync def predict(\n    request: Request,\n    file: UploadFile = File(...),\n    current_user: dict = Depends(get_current_user),\n):\n    # TODO: Implement prediction endpoint\n    # YOUR CODE HERE\n    raise HTTPException(status_code=501, detail='Not implemented yet')\n\n\nsecure_client = TestClient(secure_app)\nprint('Secure app skeleton ready. Complete the TODOs above.')"
  },
  {
   "cell_type": "code",
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Security Verification Tests ───────────────────────────────────────────────\n",
    "\n",
    "print('Running security verification...')\n",
    "results = {}\n",
    "\n",
    "# Test 1: Unauthenticated request\n",
    "buf, fname = create_test_image()\n",
    "r = secure_client.post('/predict', files={'file': (fname, buf, 'image/png')})\n",
    "results['JWT blocks unauthenticated'] = r.status_code == 401\n",
    "\n",
    "# Test 2: Wrong password\n",
    "r = secure_client.post('/token', data={'username': 'labuser', 'password': 'wrongpass'})\n",
    "results['Auth rejects wrong password'] = r.status_code == 401\n",
    "\n",
    "# Test 3: Valid login\n",
    "r = secure_client.post('/token', data={'username': 'labuser', 'password': 'labpass'})\n",
    "results['Valid login returns token'] = r.status_code == 200\n",
    "token = r.json().get('access_token', '') if r.status_code == 200 else ''\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "# Test 4: Authenticated predict\n",
    "if token:\n",
    "    buf, fname = create_test_image()\n",
    "    r = secure_client.post('/predict', headers=headers, files={'file': (fname, buf, 'image/png')})\n",
    "    results['Authenticated predict works'] = r.status_code == 200\n",
    "else:\n",
    "    results['Authenticated predict works'] = False\n",
    "\n",
    "# Test 5: Rate limiting\n",
    "rate_limiter.reset()\n",
    "statuses = []\n",
    "for _ in range(5):  # max=3, so last 2 should be 429\n",
    "    buf, fname = create_test_image()\n",
    "    r = secure_client.post('/predict', headers=headers, files={'file': (fname, buf, 'image/png')})\n",
    "    statuses.append(r.status_code)\n",
    "results['Rate limit returns 429'] = 429 in statuses\n",
    "\n",
    "# Print results\n",
    "print('\\nSecurity Verification Results:')\n",
    "for check, passed in results.items():\n",
    "    status = '✓ PASS' if passed else '✗ FAIL'\n",
    "    print(f'  {status}  {check}')\n",
    "all_pass = all(results.values())\n",
    "print(f'\\nOverall: {\"ALL PASSED\" if all_pass else \"SOME FAILED — review TODOs\"}')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Security Checklist ────────────────────────────────────────────────────────\n",
    "# TODO: For each item, set True (implemented) or False (not yet)\n",
    "# Base your answers on the secure_app you built above\n",
    "\n",
    "security_checklist = [\n",
    "    # (description, implemented: True/False, your_notes)\n",
    "    ('JWT Authentication on /predict',     None,  'TODO: set True or False'),\n",
    "    ('Rate limiting (sliding window)',      None,  'TODO: set True or False'),\n",
    "    ('File type validation',                None,  'TODO: set True or False'),\n",
    "    ('Image size validation (min 32x32)',   None,  'TODO: set True or False'),\n",
    "    ('Empty file handling',                 None,  'TODO: set True or False'),\n",
    "    ('No secrets hardcoded in source',      None,  'TODO: set True or False — check your SECRET_KEY'),\n",
    "    ('Error messages do not expose stack',  None,  'TODO: set True or False'),\n",
    "    ('HTTPS in production deployment',      None,  'TODO: set True or False'),\n",
    "]\n",
    "\n",
    "print('\\n=== Security Checklist ===')\n",
    "passed = sum(1 for _, done, _ in security_checklist if done is True)\n",
    "for item, done, note in security_checklist:\n",
    "    if done is None:\n",
    "        status = '? PENDING'\n",
    "    elif done:\n",
    "        status = '✓ DONE'\n",
    "    else:\n",
    "        status = '✗ TODO'\n",
    "    print(f'  {status}  {item:<40}  {note}')\n",
    "total = len(security_checklist)\n",
    "print(f'\\nScore: {passed}/{total} items completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Deliverable 3: Automated Test Scripts\n",
    "\n",
    "Complete the three test classes. The `client` below uses a simple (non-secure) API for testing — you can also test your `secure_client` by adding auth headers."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Simple API for tests (no auth — focus on test writing) ───────────────────\n",
    "simple_app = FastAPI(title='Lab3 Test API')\n",
    "\n",
    "@simple_app.get('/health')\n",
    "async def s_health(): return {'status': 'healthy', 'version': '3.0'}\n",
    "\n",
    "@simple_app.post('/predict')\n",
    "async def s_predict(file: UploadFile = File(...)):\n",
    "    start = time.time()\n",
    "    ext = Path(file.filename).suffix.lower() if file.filename else ''\n",
    "    if ext not in {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}:\n",
    "        raise HTTPException(status_code=400, detail=f'Invalid file type: {ext}')\n",
    "    contents = await file.read()\n",
    "    if not contents: raise HTTPException(status_code=400, detail='Empty file')\n",
    "    try: img = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "    except: raise HTTPException(status_code=400, detail='Cannot open image')\n",
    "    if img.size[0] < 32 or img.size[1] < 32:\n",
    "        raise HTTPException(status_code=400, detail='Image too small')\n",
    "    tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(tensor); probs = torch.softmax(out, 1); conf, idx = torch.max(probs,1)\n",
    "    lat = (time.time() - start) * 1000\n",
    "    class_probs = {CLASS_NAMES[i]: float(probs[0,i]) for i in range(NUM_CLASSES)}\n",
    "    return {'prediction': CLASS_NAMES[idx.item()], 'confidence': round(float(conf.item()),4),\n",
    "            'class_probabilities': class_probs, 'model_version': 'v3.0-lab', 'latency_ms': round(lat,2)}\n",
    "\n",
    "client = TestClient(simple_app)\n",
    "print('Simple test API ready.')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Deliverable 3: Test Class 1 — API Response Tests ─────────────────────────\n",
    "\n",
    "class TestAPIResponse:\n",
    "    \"\"\"Tests for API endpoint correctness and error handling.\"\"\"\n",
    "\n",
    "    def test_health_endpoint_returns_200(self):\n",
    "        # TODO: Call GET /health and assert status_code == 200\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_predict_returns_200_for_valid_image(self):\n",
    "        # TODO: Create a test image, POST to /predict, assert 200\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_predict_response_has_prediction_field(self):\n",
    "        # TODO: POST valid image, assert 'prediction' key in json response\n",
    "        # AND assert prediction is in CLASS_NAMES\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_predict_class_probabilities_sum_to_one(self):\n",
    "        # TODO: POST valid image, sum class_probabilities values, assert ≈ 1.0\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_invalid_file_type_returns_400(self):\n",
    "        # TODO: POST a .txt file, assert status_code == 400\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_corrupt_image_returns_400(self):\n",
    "        # TODO: POST random bytes as .png, assert status_code == 400\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_too_small_image_returns_400(self):\n",
    "        # TODO: POST a 16x16 image, assert status_code == 400\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "\n",
    "print('TestAPIResponse defined (complete the TODO methods).')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Test Class 2 — Accuracy Threshold Tests ──────────────────────────────────\n",
    "\n",
    "# NOTE: With random weights, accuracy ≈ 14% (1/7 chance level)\n",
    "# Set threshold to 10% so tests pass even with untrained model\n",
    "# In production with a trained model: set ACCURACY_THRESHOLD = 70.0\n",
    "ACCURACY_THRESHOLD = 10.0\n",
    "\n",
    "class TestAccuracyThreshold:\n",
    "    \"\"\"Model quality gate tests — block deployment if accuracy is too low.\"\"\"\n",
    "\n",
    "    def test_overall_accuracy_above_threshold(self):\n",
    "        # TODO: Load ImageFolder from TEST_PATH, create DataLoader,\n",
    "        #       evaluate model accuracy, assert >= ACCURACY_THRESHOLD\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_model_is_deterministic(self):\n",
    "        # TODO: Call /predict 5 times with the same image\n",
    "        #       Assert all predictions are identical\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_confidence_values_in_valid_range(self):\n",
    "        # TODO: Call /predict, assert 0.0 <= confidence <= 1.0\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "\n",
    "print('TestAccuracyThreshold defined (complete the TODO methods).')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Test Class 3 — Latency Threshold Tests ───────────────────────────────────\n",
    "\n",
    "SINGLE_LATENCY_MS  = 200   # ms — conservative SLA for CPU demo\n",
    "LATENCY_P99_SLA_MS = 500   # ms — p99 across 50 requests\n",
    "\n",
    "class TestLatencyThreshold:\n",
    "    \"\"\"Performance SLA tests — block deployment if latency is too high.\"\"\"\n",
    "\n",
    "    def test_single_request_under_sla(self):\n",
    "        # TODO: Time a single /predict call (wall-clock)\n",
    "        #       Assert elapsed_ms < SINGLE_LATENCY_MS\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_p99_latency_under_sla(self):\n",
    "        # TODO: Make 50 requests, collect latencies\n",
    "        #       Compute p99 = np.percentile(latencies, 99)\n",
    "        #       Assert p99 < LATENCY_P99_SLA_MS\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "    def test_api_reported_latency_positive(self):\n",
    "        # TODO: POST image, assert response.json()['latency_ms'] > 0\n",
    "        # YOUR CODE HERE\n",
    "        pass  # TODO\n",
    "\n",
    "\n",
    "print('TestLatencyThreshold defined (complete the TODO methods).')"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Run All Tests ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def run_test_class(test_class):\n",
    "    instance = test_class()\n",
    "    results  = []\n",
    "    methods  = [m for m in dir(instance) if m.startswith('test_')]\n",
    "    for name in sorted(methods):\n",
    "        try:\n",
    "            getattr(instance, name)()\n",
    "            results.append((name, 'PASS', None))\n",
    "        except (AssertionError, NotImplementedError, TypeError) as e:\n",
    "            results.append((name, 'FAIL/TODO', str(e)[:60]))\n",
    "        except Exception as e:\n",
    "            results.append((name, 'ERROR', str(e)[:60]))\n",
    "    passed = sum(1 for _, s, _ in results if s == 'PASS')\n",
    "    return passed, len(results) - passed, results\n",
    "\n",
    "print('=' * 65)\n",
    "print('  Lab 3 — Test Results')\n",
    "print('=' * 65)\n",
    "total_p = total_f = 0\n",
    "for cls in [TestAPIResponse, TestAccuracyThreshold, TestLatencyThreshold]:\n",
    "    p, f, r = run_test_class(cls)\n",
    "    total_p += p; total_f += f\n",
    "    status = '✓ ALL PASS' if f == 0 else f'✗ {f} FAIL/TODO'\n",
    "    print(f'\\n  {cls.__name__}: {p}/{p+f} passed  [{status}]')\n",
    "    for name, state, err in r:\n",
    "        icon = '  ✓' if state == 'PASS' else '  ✗'\n",
    "        print(f'{icon} {name}')\n",
    "        if err: print(f'       → {err}')\n",
    "print(f'\\n  TOTAL: {total_p} passed, {total_f} fail/todo')\n",
    "print('=' * 65)\n",
    "print('\\nComplete all TODO methods to get 13/13 passing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Final: Production Checklist\n",
    "\n",
    "Fill in the `status` column for each item based on your work above."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Final Checklist ───────────────────────────────────────────────────────────\n",
    "# TODO: Update each status to 'PASS', 'FAIL', or 'PARTIAL' based on your work\n",
    "\n",
    "checklist = [\n",
    "    # (Category, Item, Status, Notes)\n",
    "    ('Performance',  'Latency measured (p50/p95/p99)',      'TODO', ''),\n",
    "    ('Performance',  'Throughput measured (img/s)',          'TODO', ''),\n",
    "    ('Performance',  'Dynamic quantization applied',         'TODO', ''),\n",
    "    ('Performance',  'Benchmark chart saved (PNG)',          'TODO', ''),\n",
    "    ('Security',     'JWT authentication on /predict',       'TODO', ''),\n",
    "    ('Security',     'Rate limiting (HTTP 429)',             'TODO', ''),\n",
    "    ('Security',     'Input validation (type/size)',         'TODO', ''),\n",
    "    ('Security',     'Security checklist 6/8 completed',    'TODO', ''),\n",
    "    ('Testing',      'API response tests written (7)',       'TODO', ''),\n",
    "    ('Testing',      'Accuracy threshold test written',      'TODO', ''),\n",
    "    ('Testing',      'Latency SLA test written',             'TODO', ''),\n",
    "    ('Testing',      'All tests pass',                       'TODO', ''),\n",
    "    ('Quantization', 'Size comparison table shown',          'TODO', ''),\n",
    "    ('Quantization', 'Accuracy delta computed',              'TODO', ''),\n",
    "]\n",
    "\n",
    "df_checklist = pd.DataFrame(checklist, columns=['Category', 'Item', 'Status', 'Notes'])\n",
    "df_checklist.index += 1\n",
    "\n",
    "print('=== Production Testing Checklist ===')\n",
    "print(df_checklist.to_string())\n",
    "\n",
    "passed_items = (df_checklist['Status'] == 'PASS').sum()\n",
    "print(f'\\nScore: {passed_items}/{len(checklist)} items PASS')\n",
    "print('Replace TODO with PASS/FAIL/PARTIAL as you complete each item.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Requirements\n",
    "\n",
    "Before submitting, verify:\n",
    "\n",
    "- [ ] All `# TODO:` cells are completed with working code\n",
    "- [ ] `lab3_benchmark.png` was generated successfully\n",
    "- [ ] Security verification shows all 5 checks passing\n",
    "- [ ] All 13 test methods pass in the test runner output\n",
    "- [ ] Quantization comparison table is filled in\n",
    "- [ ] Final checklist shows at least 10/14 items as PASS\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck!** Refer to the Part notebooks for guidance. The solution notebook (`Lab_3_Production_Testing_Solution.ipynb`) is available after submission."
   ]
  }
 ]
}