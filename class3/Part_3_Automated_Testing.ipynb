{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Part 3: Automated Testing\n",
    "\n",
    "**Class 3 â€” Non-Functional Testing & AI Security**\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "1. Write **pytest scripts** for three test categories:\n",
    "   - API response tests (correctness & error handling)\n",
    "   - Accuracy threshold tests (model quality gate)\n",
    "   - Latency threshold tests (performance SLA)\n",
    "2. Understand **Continuous Integration (CI)** and how automated tests gate deployment\n",
    "3. See how an **automated ML testing pipeline** works end-to-end\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|------------|\n",
    "| **pytest** | Python test framework â€” auto-discovers `test_*.py` files and `test_*` functions |\n",
    "| **Fixture** | Reusable test setup/teardown declared with `@pytest.fixture` |\n",
    "| **Assertion** | `assert <condition>` â€” test fails if condition is False |\n",
    "| **CI/CD** | Continuous Integration/Delivery â€” run tests on every code push |\n",
    "| **Quality Gate** | Minimum passing criteria before code can merge to main |\n",
    "| **SLA** | Service Level Agreement â€” e.g., p99 latency < 200ms |\n",
    "\n",
    "---\n",
    "\n",
    "## The Testing Pyramid for ML Systems\n",
    "\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  End-to-End      â”‚  â† Slow, few: full prediction workflow\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  Integration     â”‚  â† Medium: multi-model, canary, metrics\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  API Response    â”‚  â† Fast, many: endpoint correctness\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  Unit: Accuracy  â”‚  â† Model quality gates\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  Unit: Latency   â”‚  â† Performance SLA gates\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Class 3 focuses on the **API Response**, **Accuracy**, and **Latency** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Test thresholds: accuracy>10.0%, latency p99<500ms\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 1: Setup & Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Request\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "from jose import JWTError, jwt\n",
    "import bcrypt as _bcrypt\n",
    "\n",
    "# Constants\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CLASS_NAMES = ['animal', 'name_board', 'other_vehicle', 'pedestrian',\n",
    "               'pothole', 'road_sign', 'speed_breaker']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "DATASET_PATH = r'C:\\Users\\Lucifer\\python_workspace\\BITS\\AI_Quality_Engineering\\dataset'\n",
    "TEST_PATH = os.path.join(DATASET_PATH, 'test')\n",
    "\n",
    "# Test thresholds\n",
    "# NOTE: These use values compatible with RANDOM weights (no training).\n",
    "# In production with a trained model, set:\n",
    "#   ACCURACY_THRESHOLD = 70.0  (trained ResNet-18 achieves ~83%)\n",
    "#   LATENCY_P99_SLA_MS = 100   (tighter SLA for production)\n",
    "ACCURACY_THRESHOLD  = 10.0   # Random weights â†’ ~14% (chance level)\n",
    "LATENCY_P99_SLA_MS  = 500    # ms â€” conservative for CPU demo\n",
    "SINGLE_LATENCY_MS   = 200    # ms â€” per-request SLA\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'Test thresholds: accuracy>{ACCURACY_THRESHOLD}%, latency p99<{LATENCY_P99_SLA_MS}ms')\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucifer\\anaconda3\\envs\\venv_ai_quality\\lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_version\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API and TestClient ready.\n",
      "Auth token acquired: eyJhbGciOiJIUzI1NiIsInR5cCI6Ik...\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 2: Model & API Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Model definition (same as class2)\n",
    "class ADASModel(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(weights=None)\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)\n",
    "    def forward(self, x): return self.resnet(x)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "model = ADASModel(NUM_CLASSES).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# --- JWT setup (from Part 2) ---\n",
    "SECRET_KEY = 'adas-demo-secret-key-change-in-production'\n",
    "ALGORITHM  = 'HS256'\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n",
    "FAKE_USERS_DB = {\n",
    "    'testuser': {'username': 'testuser',\n",
    "                 'hashed_password': _bcrypt.hashpw(b'testpassword', _bcrypt.gensalt()),\n",
    "                 'disabled': False}\n",
    "}\n",
    "\n",
    "def create_access_token(data, expires_delta=None):\n",
    "    to_encode = data.copy()\n",
    "    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))\n",
    "    to_encode.update({'exp': expire})\n",
    "    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme)):\n",
    "    try:\n",
    "        payload  = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        username = payload.get('sub')\n",
    "        if username is None: raise HTTPException(status_code=401, detail='Invalid token')\n",
    "    except JWTError:\n",
    "        raise HTTPException(status_code=401, detail='Invalid token',\n",
    "                            headers={'WWW-Authenticate': 'Bearer'})\n",
    "    user = FAKE_USERS_DB.get(username)\n",
    "    if not user: raise HTTPException(status_code=401, detail='User not found')\n",
    "    return user\n",
    "\n",
    "# Pydantic schemas\n",
    "class Token(BaseModel):\n",
    "    access_token: str; token_type: str\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str; confidence: float\n",
    "    class_probabilities: Dict[str, float]\n",
    "    model_version: str; latency_ms: float\n",
    "\n",
    "# --- FastAPI app ---\n",
    "app = FastAPI(title='ADAS Test API', version='3.0.0')\n",
    "\n",
    "@app.post('/token', response_model=Token)\n",
    "async def login(form_data: OAuth2PasswordRequestForm = Depends()):\n",
    "    user = FAKE_USERS_DB.get(form_data.username)\n",
    "    if not user or not _bcrypt.checkpw(form_data.password.encode('utf-8'), user['hashed_password']):\n",
    "        raise HTTPException(status_code=401, detail='Invalid credentials')\n",
    "    token = create_access_token({'sub': user['username']}, timedelta(minutes=30))\n",
    "    return {'access_token': token, 'token_type': 'bearer'}\n",
    "\n",
    "@app.get('/health')\n",
    "async def health(): return {'status': 'healthy', 'version': '3.0.0', 'device': str(DEVICE)}\n",
    "\n",
    "@app.post('/predict', response_model=PredictionResponse)\n",
    "async def predict(file: UploadFile = File(...), current_user: dict = Depends(get_current_user)):\n",
    "    start = time.time()\n",
    "    allowed = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n",
    "    ext = Path(file.filename).suffix.lower() if file.filename else ''\n",
    "    if ext not in allowed:\n",
    "        raise HTTPException(status_code=400, detail=f'Invalid file type: {ext}')\n",
    "    contents = await file.read()\n",
    "    if not contents: raise HTTPException(status_code=400, detail='Empty file')\n",
    "    try: image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "    except Exception as e: raise HTTPException(status_code=400, detail=f'Invalid image: {e}')\n",
    "    if image.size[0] < 32 or image.size[1] < 32:\n",
    "        raise HTTPException(status_code=400, detail='Image too small (min 32x32)')\n",
    "    tensor = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out   = model(tensor)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        conf, idx = torch.max(probs, 1)\n",
    "    lat = (time.time() - start) * 1000\n",
    "    return PredictionResponse(\n",
    "        prediction=CLASS_NAMES[idx.item()], confidence=round(float(conf.item()),4),\n",
    "        class_probabilities={CLASS_NAMES[i]: float(probs[0,i]) for i in range(NUM_CLASSES)},\n",
    "        model_version='v3.0', latency_ms=round(lat,2))\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "# Pre-acquire token for authenticated tests\n",
    "token_resp = client.post('/token', data={'username': 'testuser', 'password': 'testpassword'})\n",
    "AUTH_TOKEN  = token_resp.json()['access_token']\n",
    "AUTH_HEADERS = {'Authorization': f'Bearer {AUTH_TOKEN}'}\n",
    "\n",
    "print('API and TestClient ready.')\n",
    "print(f'Auth token acquired: {AUTH_TOKEN[:30]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-04",
   "metadata": {},
   "source": [
    "## Section 3: Test Helpers\n",
    "\n",
    "Helper functions used across all test classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper sanity check: 200 â†’ pothole\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 3: Test Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def create_test_image(size=(100, 100), color='red', fmt='PNG'):\n",
    "    \"\"\"Create an in-memory test image. Returns (BytesIO, filename).\"\"\"\n",
    "    img = Image.new('RGB', size, color=color)\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=fmt)\n",
    "    buf.seek(0)\n",
    "    return buf, f'test.{fmt.lower()}'\n",
    "\n",
    "\n",
    "def post_predict(image_buf, filename='test.png'):\n",
    "    \"\"\"POST to /predict with authentication.\"\"\"\n",
    "    return client.post(\n",
    "        '/predict',\n",
    "        headers=AUTH_HEADERS,\n",
    "        files={'file': (filename, image_buf, 'image/png')},\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model_accuracy(eval_model, loader, device):\n",
    "    \"\"\"Evaluate accuracy on a DataLoader. Returns percentage.\"\"\"\n",
    "    eval_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            _, preds = torch.max(eval_model(images), 1)\n",
    "            correct += (preds.cpu() == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    return round(100 * correct / total, 2) if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# Test helper functions\n",
    "buf, fname = create_test_image()\n",
    "resp = post_predict(buf, fname)\n",
    "print(f'Helper sanity check: {resp.status_code} â†’ {resp.json().get(\"prediction\", \"error\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-06",
   "metadata": {},
   "source": [
    "## Section 4: Category 1 â€” API Response Tests\n",
    "\n",
    "These tests verify that the API:\n",
    "- Returns correct HTTP status codes\n",
    "- Includes required fields in responses\n",
    "- Handles errors gracefully\n",
    "- Enforces security (401 without token)\n",
    "\n",
    "**pytest convention**: Test methods start with `test_`, organized in classes starting with `Test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestAPIResponse class defined with 16 tests.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 4: API Response Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class TestAPIResponse:\n",
    "    \"\"\"\n",
    "    Tests for API endpoint correctness, error handling, and security.\n",
    "    Category 1 of 3 in our automated test suite.\n",
    "    \"\"\"\n",
    "\n",
    "    # -- Health endpoint --\n",
    "\n",
    "    def test_health_endpoint_returns_200(self):\n",
    "        resp = client.get('/health')\n",
    "        assert resp.status_code == 200, f'Expected 200, got {resp.status_code}'\n",
    "\n",
    "    def test_health_response_has_status_field(self):\n",
    "        resp = client.get('/health')\n",
    "        assert 'status' in resp.json(), 'Missing status field'\n",
    "        assert resp.json()['status'] == 'healthy'\n",
    "\n",
    "    def test_health_response_has_version_field(self):\n",
    "        resp = client.get('/health')\n",
    "        assert 'version' in resp.json(), 'Missing version field'\n",
    "\n",
    "    # -- Prediction endpoint - success cases --\n",
    "\n",
    "    def test_predict_returns_200_for_valid_image(self):\n",
    "        buf, fname = create_test_image()\n",
    "        resp = post_predict(buf, fname)\n",
    "        assert resp.status_code == 200, f'Expected 200, got {resp.status_code}: {resp.text}'\n",
    "\n",
    "    def test_predict_response_includes_prediction_field(self):\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        assert 'prediction' in data, 'Response missing prediction field'\n",
    "        assert data['prediction'] in CLASS_NAMES, \\\n",
    "            f'Unknown prediction: {data[\"prediction\"]}'\n",
    "\n",
    "    def test_predict_response_includes_confidence(self):\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        assert 'confidence' in data\n",
    "        assert 0.0 <= data['confidence'] <= 1.0, \\\n",
    "            f'Confidence {data[\"confidence\"]} out of range [0, 1]'\n",
    "\n",
    "    def test_predict_class_probabilities_sum_to_one(self):\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        assert 'class_probabilities' in data\n",
    "        total = sum(data['class_probabilities'].values())\n",
    "        assert abs(total - 1.0) < 0.01, \\\n",
    "            f'Probabilities sum to {total:.4f}, expected 1.0'\n",
    "\n",
    "    def test_predict_returns_all_class_names(self):\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        for cls in CLASS_NAMES:\n",
    "            assert cls in data['class_probabilities'], \\\n",
    "                f'Class {cls} missing from probabilities'\n",
    "\n",
    "    def test_predict_includes_latency_ms(self):\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        assert 'latency_ms' in data\n",
    "        assert data['latency_ms'] > 0, 'Latency must be positive'\n",
    "\n",
    "    # -- Error handling --\n",
    "\n",
    "    def test_invalid_file_type_returns_400(self):\n",
    "        txt_file = io.BytesIO(b'this is not an image')\n",
    "        resp = client.post(\n",
    "            '/predict', headers=AUTH_HEADERS,\n",
    "            files={'file': ('document.txt', txt_file, 'text/plain')},\n",
    "        )\n",
    "        assert resp.status_code == 400, f'Expected 400, got {resp.status_code}'\n",
    "\n",
    "    def test_corrupt_image_returns_400(self):\n",
    "        corrupt = io.BytesIO(b'this-is-not-a-valid-png-header')\n",
    "        resp = client.post(\n",
    "            '/predict', headers=AUTH_HEADERS,\n",
    "            files={'file': ('corrupt.png', corrupt, 'image/png')},\n",
    "        )\n",
    "        assert resp.status_code == 400\n",
    "\n",
    "    def test_too_small_image_returns_400(self):\n",
    "        buf, fname = create_test_image(size=(16, 16))  # Below 32x32 minimum\n",
    "        resp = post_predict(buf, fname)\n",
    "        assert resp.status_code == 400, f'Expected 400 for tiny image, got {resp.status_code}'\n",
    "\n",
    "    def test_empty_file_returns_400(self):\n",
    "        empty = io.BytesIO(b'')\n",
    "        resp = client.post(\n",
    "            '/predict', headers=AUTH_HEADERS,\n",
    "            files={'file': ('empty.png', empty, 'image/png')},\n",
    "        )\n",
    "        assert resp.status_code == 400\n",
    "\n",
    "    # -- Security: authentication --\n",
    "\n",
    "    def test_unauthenticated_request_returns_401(self):\n",
    "        buf, fname = create_test_image()\n",
    "        resp = client.post('/predict', files={'file': (fname, buf, 'image/png')})\n",
    "        assert resp.status_code == 401, \\\n",
    "            f'Expected 401 without token, got {resp.status_code}'\n",
    "\n",
    "    def test_wrong_credentials_returns_401(self):\n",
    "        resp = client.post('/token', data={'username': 'testuser', 'password': 'wrong'})\n",
    "        assert resp.status_code == 401\n",
    "\n",
    "    def test_valid_credentials_return_token(self):\n",
    "        resp = client.post('/token', data={'username': 'testuser', 'password': 'testpassword'})\n",
    "        assert resp.status_code == 200\n",
    "        data = resp.json()\n",
    "        assert 'access_token' in data\n",
    "        assert data['token_type'] == 'bearer'\n",
    "\n",
    "\n",
    "print('TestAPIResponse class defined with 16 tests.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "## Section 5: Category 2 â€” Accuracy Threshold Tests\n",
    "\n",
    "These tests verify that the model meets a **minimum accuracy requirement** before deployment.\n",
    "\n",
    "This is a **quality gate** â€” if accuracy drops below the threshold, the CI pipeline fails and the code cannot merge.\n",
    "\n",
    "> **Note on thresholds**: We use `ACCURACY_THRESHOLD = 10%` here because we're using random weights (chance level â‰ˆ 14.3% for 7 classes). In production with a **trained model** (ResNet-18 achieves ~83%), set `ACCURACY_THRESHOLD = 70.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestAccuracyThreshold class defined with 5 tests.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 5: Accuracy Threshold Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class TestAccuracyThreshold:\n",
    "    \"\"\"\n",
    "    Model quality gate tests.\n",
    "    These tests BLOCK deployment if accuracy falls below threshold.\n",
    "\n",
    "    Threshold values:\n",
    "      - Demo (random weights): 10%  (chance level = 14.3%)\n",
    "      - Production (trained):  70%  (trained ResNet-18 achieves ~83%)\n",
    "    \"\"\"\n",
    "\n",
    "    # Shared test dataset (loaded once)\n",
    "    _test_loader = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_test_loader(cls):\n",
    "        if cls._test_loader is None:\n",
    "            test_ds = ImageFolder(TEST_PATH, transform=transform)\n",
    "            cls._test_loader = DataLoader(\n",
    "                test_ds, batch_size=32, shuffle=False, num_workers=0\n",
    "            )\n",
    "        return cls._test_loader\n",
    "\n",
    "    def test_overall_accuracy_above_threshold(self):\n",
    "        \"\"\"\n",
    "        Overall accuracy on test set must exceed ACCURACY_THRESHOLD.\n",
    "        PRODUCTION: Set ACCURACY_THRESHOLD = 70.0\n",
    "        \"\"\"\n",
    "        loader = self.get_test_loader()\n",
    "        accuracy = evaluate_model_accuracy(model, loader, DEVICE)\n",
    "        print(f'  Overall accuracy: {accuracy:.2f}% (threshold: {ACCURACY_THRESHOLD}%)')\n",
    "        assert accuracy >= ACCURACY_THRESHOLD, \\\n",
    "            f'Accuracy {accuracy:.2f}% below threshold {ACCURACY_THRESHOLD}%'\n",
    "\n",
    "    def test_per_class_accuracy_not_zero(self):\n",
    "        \"\"\"\n",
    "        Each class must be predicted at least once on the test set.\n",
    "        Catches cases where the model always predicts a single class.\n",
    "        \"\"\"\n",
    "        loader = self.get_test_loader()\n",
    "        predicted_classes = set()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in loader:\n",
    "                _, preds = torch.max(model(images.to(DEVICE)), 1)\n",
    "                predicted_classes.update(preds.cpu().numpy().tolist())\n",
    "        unique_preds = len(predicted_classes)\n",
    "        print(f'  Unique classes predicted: {unique_preds}/{NUM_CLASSES}')\n",
    "        assert unique_preds > 1, \\\n",
    "            f'Model only predicts {unique_preds} class(es) â€” possible degenerate model'\n",
    "\n",
    "    def test_confidence_values_in_valid_range(self):\n",
    "        \"\"\"\n",
    "        All class probability values must be in [0, 1] and sum to ~1.0.\n",
    "        Validates that softmax is applied correctly.\n",
    "        \"\"\"\n",
    "        for color in ['red', 'green', 'blue', 'white']:\n",
    "            buf, fname = create_test_image(color=color)\n",
    "            data = post_predict(buf, fname).json()\n",
    "            assert 0.0 <= data['confidence'] <= 1.0, \\\n",
    "                f'Confidence {data[\"confidence\"]} out of range'\n",
    "            total = sum(data['class_probabilities'].values())\n",
    "            assert abs(total - 1.0) < 0.01, \\\n",
    "                f'Probabilities sum to {total:.4f} for {color} image'\n",
    "\n",
    "    def test_model_prediction_deterministic(self):\n",
    "        \"\"\"\n",
    "        Same image must always produce the same prediction (no randomness).\n",
    "        Verifies model is in eval() mode (no dropout active).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _ in range(5):\n",
    "            buf, fname = create_test_image(color='purple', size=(128, 128))\n",
    "            data = post_predict(buf, fname).json()\n",
    "            predictions.append(data['prediction'])\n",
    "        assert len(set(predictions)) == 1, \\\n",
    "            f'Non-deterministic predictions: {predictions}'\n",
    "\n",
    "    def test_prediction_is_valid_class(self):\n",
    "        \"\"\"Prediction must always be one of the 7 known class names.\"\"\"\n",
    "        for _ in range(3):\n",
    "            buf, fname = create_test_image()\n",
    "            data = post_predict(buf, fname).json()\n",
    "            assert data['prediction'] in CLASS_NAMES, \\\n",
    "                f'Unknown prediction class: {data[\"prediction\"]}'\n",
    "\n",
    "\n",
    "print('TestAccuracyThreshold class defined with 5 tests.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Section 6: Category 3 â€” Latency Threshold Tests\n",
    "\n",
    "These tests verify that the API meets **performance SLA** requirements.\n",
    "\n",
    "**Why test latency?** An ADAS car traveling at 60 km/h moves 16 meters/second. A 500ms delay means the car has moved 8 meters without updated hazard detection.\n",
    "\n",
    "**SLA values used here** (for CPU demo with random weights):\n",
    "- Single request: < 200ms\n",
    "- p99 across 50 requests: < 500ms\n",
    "\n",
    "In production: tighten to <50ms single, <100ms p99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestLatencyThreshold class defined with 5 tests.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 6: Latency Threshold Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class TestLatencyThreshold:\n",
    "    \"\"\"\n",
    "    Performance SLA tests.\n",
    "    Verify that inference meets latency requirements for production deployment.\n",
    "\n",
    "    SLA values (CPU demo):\n",
    "      - Single request: < 200ms\n",
    "      - p99 (50 requests): < 500ms\n",
    "    PRODUCTION: Tighten to 50ms single, 100ms p99 with GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def test_single_image_latency_under_sla(self):\n",
    "        \"\"\"\n",
    "        A single prediction request (wall-clock) must complete within SINGLE_LATENCY_MS.\n",
    "        This includes: HTTP overhead + image parsing + inference.\n",
    "        \"\"\"\n",
    "        buf, fname = create_test_image(size=(128, 128))\n",
    "        start   = time.perf_counter()\n",
    "        resp    = post_predict(buf, fname)\n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        assert resp.status_code == 200\n",
    "        print(f'  Single request latency: {elapsed:.2f}ms (SLA: <{SINGLE_LATENCY_MS}ms)')\n",
    "        assert elapsed < SINGLE_LATENCY_MS, \\\n",
    "            f'Latency {elapsed:.2f}ms exceeds SLA of {SINGLE_LATENCY_MS}ms'\n",
    "\n",
    "    def test_p99_latency_under_sla(self):\n",
    "        \"\"\"\n",
    "        p99 latency across 50 requests must be under LATENCY_P99_SLA_MS.\n",
    "        Tail latency matters: 1% of users experiencing > 500ms is unacceptable in ADAS.\n",
    "        \"\"\"\n",
    "        n_requests = 50\n",
    "        latencies  = []\n",
    "        for _ in range(n_requests):\n",
    "            buf, fname = create_test_image()\n",
    "            start   = time.perf_counter()\n",
    "            resp    = post_predict(buf, fname)\n",
    "            elapsed = (time.perf_counter() - start) * 1000\n",
    "            assert resp.status_code == 200\n",
    "            latencies.append(elapsed)\n",
    "\n",
    "        p50 = np.percentile(latencies, 50)\n",
    "        p95 = np.percentile(latencies, 95)\n",
    "        p99 = np.percentile(latencies, 99)\n",
    "        print(f'  p50={p50:.1f}ms  p95={p95:.1f}ms  p99={p99:.1f}ms  '\n",
    "              f'(SLA: p99<{LATENCY_P99_SLA_MS}ms)')\n",
    "        assert p99 < LATENCY_P99_SLA_MS, \\\n",
    "            f'p99 latency {p99:.2f}ms exceeds SLA of {LATENCY_P99_SLA_MS}ms'\n",
    "\n",
    "    def test_latency_reported_by_api_is_positive(self):\n",
    "        \"\"\"\n",
    "        The latency_ms field in the API response must be > 0.\n",
    "        Checks that we haven't accidentally disabled timing.\n",
    "        \"\"\"\n",
    "        buf, fname = create_test_image()\n",
    "        data = post_predict(buf, fname).json()\n",
    "        assert data['latency_ms'] > 0, 'API-reported latency must be positive'\n",
    "\n",
    "    def test_large_image_within_sla(self):\n",
    "        \"\"\"\n",
    "        Large images (640x480) should still complete within 2Ã— the single SLA.\n",
    "        Resize transform makes input size uniform â€” latency should not blow up.\n",
    "        \"\"\"\n",
    "        buf, fname = create_test_image(size=(640, 480))\n",
    "        start   = time.perf_counter()\n",
    "        resp    = post_predict(buf, fname)\n",
    "        elapsed = (time.perf_counter() - start) * 1000\n",
    "        assert resp.status_code == 200\n",
    "        sla_large = SINGLE_LATENCY_MS * 2\n",
    "        print(f'  640x480 image latency: {elapsed:.2f}ms (SLA: <{sla_large}ms)')\n",
    "        assert elapsed < sla_large, \\\n",
    "            f'Large image latency {elapsed:.2f}ms exceeds 2x SLA of {sla_large}ms'\n",
    "\n",
    "    def test_successive_requests_consistent_latency(self):\n",
    "        \"\"\"\n",
    "        Latency must not degrade significantly over successive requests.\n",
    "        Checks for memory leaks or state accumulation.\n",
    "        \"\"\"\n",
    "        latencies = []\n",
    "        for _ in range(20):\n",
    "            buf, fname = create_test_image()\n",
    "            start = time.perf_counter()\n",
    "            post_predict(buf, fname)\n",
    "            latencies.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "        first_half_mean = np.mean(latencies[:10])\n",
    "        second_half_mean = np.mean(latencies[10:])\n",
    "        degradation_pct = ((second_half_mean - first_half_mean) / first_half_mean) * 100\n",
    "        print(f'  Latency: first 10 mean={first_half_mean:.1f}ms, '\n",
    "              f'last 10 mean={second_half_mean:.1f}ms, '\n",
    "              f'degradation={degradation_pct:.1f}%')\n",
    "        # Allow up to 50% degradation (warming up effects on cold start)\n",
    "        assert degradation_pct < 50, \\\n",
    "            f'Latency degraded by {degradation_pct:.1f}% over 20 requests (possible memory leak)'\n",
    "\n",
    "\n",
    "print('TestLatencyThreshold class defined with 5 tests.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Section 7: Run All Tests\n",
    "\n",
    "We run all three test categories and print a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  Automated Test Suite â€” Class 3 Part 3\n",
      "=================================================================\n",
      "\n",
      "  TestAPIResponse: 16/16 passed  [âœ“ ALL PASS]\n",
      "  âœ“ test_corrupt_image_returns_400\n",
      "  âœ“ test_empty_file_returns_400\n",
      "  âœ“ test_health_endpoint_returns_200\n",
      "  âœ“ test_health_response_has_status_field\n",
      "  âœ“ test_health_response_has_version_field\n",
      "  âœ“ test_invalid_file_type_returns_400\n",
      "  âœ“ test_predict_class_probabilities_sum_to_one\n",
      "  âœ“ test_predict_includes_latency_ms\n",
      "  âœ“ test_predict_response_includes_confidence\n",
      "  âœ“ test_predict_response_includes_prediction_field\n",
      "  âœ“ test_predict_returns_200_for_valid_image\n",
      "  âœ“ test_predict_returns_all_class_names\n",
      "  âœ“ test_too_small_image_returns_400\n",
      "  âœ“ test_unauthenticated_request_returns_401\n",
      "  âœ“ test_valid_credentials_return_token\n",
      "  âœ“ test_wrong_credentials_returns_401\n",
      "  Overall accuracy: 23.93% (threshold: 10.0%)\n",
      "  Unique classes predicted: 1/7\n",
      "\n",
      "  TestAccuracyThreshold: 4/5 passed  [âœ— 1 FAILED]\n",
      "  âœ“ test_confidence_values_in_valid_range\n",
      "  âœ“ test_model_prediction_deterministic\n",
      "  âœ“ test_overall_accuracy_above_threshold\n",
      "  âœ— test_per_class_accuracy_not_zero\n",
      "     â†’ Model only predicts 1 class(es) â€” possible degenerate model\n",
      "  âœ“ test_prediction_is_valid_class\n",
      "  640x480 image latency: 54.98ms (SLA: <400ms)\n",
      "  p50=39.8ms  p95=79.0ms  p99=133.4ms  (SLA: p99<500ms)\n",
      "  Single request latency: 39.39ms (SLA: <200ms)\n",
      "  Latency: first 10 mean=38.7ms, last 10 mean=40.8ms, degradation=5.4%\n",
      "\n",
      "  TestLatencyThreshold: 5/5 passed  [âœ“ ALL PASS]\n",
      "  âœ“ test_large_image_within_sla\n",
      "  âœ“ test_latency_reported_by_api_is_positive\n",
      "  âœ“ test_p99_latency_under_sla\n",
      "  âœ“ test_single_image_latency_under_sla\n",
      "  âœ“ test_successive_requests_consistent_latency\n",
      "\n",
      "=================================================================\n",
      "  TOTAL: 25 passed, 1 failed\n",
      "  1 TESTS FAILED âœ—\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 7: Test Runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def run_test_class(test_class):\n",
    "    \"\"\"\n",
    "    Run all test_* methods in a test class.\n",
    "    Returns (passed, failed, results_list).\n",
    "    \"\"\"\n",
    "    instance = test_class()\n",
    "    results  = []\n",
    "    methods  = [m for m in dir(instance) if m.startswith('test_')]\n",
    "\n",
    "    for method_name in sorted(methods):\n",
    "        method = getattr(instance, method_name)\n",
    "        try:\n",
    "            method()\n",
    "            results.append((method_name, 'PASS', None))\n",
    "        except AssertionError as e:\n",
    "            results.append((method_name, 'FAIL', str(e)))\n",
    "        except Exception as e:\n",
    "            results.append((method_name, 'ERROR', str(e)))\n",
    "\n",
    "    passed = sum(1 for _, s, _ in results if s == 'PASS')\n",
    "    failed = len(results) - passed\n",
    "    return passed, failed, results\n",
    "\n",
    "\n",
    "def print_test_results(class_name, passed, failed, results):\n",
    "    total = passed + failed\n",
    "    status = 'âœ“ ALL PASS' if failed == 0 else f'âœ— {failed} FAILED'\n",
    "    print(f'\\n  {class_name}: {passed}/{total} passed  [{status}]')\n",
    "    for name, state, err in results:\n",
    "        icon = '  âœ“' if state == 'PASS' else '  âœ—'\n",
    "        print(f'{icon} {name}')\n",
    "        if err: print(f'     â†’ {err[:80]}')\n",
    "\n",
    "\n",
    "# Run all test categories\n",
    "print('=' * 65)\n",
    "print('  Automated Test Suite â€” Class 3 Part 3')\n",
    "print('=' * 65)\n",
    "\n",
    "total_pass = total_fail = 0\n",
    "\n",
    "for cls in [TestAPIResponse, TestAccuracyThreshold, TestLatencyThreshold]:\n",
    "    p, f, r = run_test_class(cls)\n",
    "    print_test_results(cls.__name__, p, f, r)\n",
    "    total_pass += p\n",
    "    total_fail += f\n",
    "\n",
    "print('\\n' + '=' * 65)\n",
    "print(f'  TOTAL: {total_pass} passed, {total_fail} failed')\n",
    "overall = 'ALL TESTS PASSED âœ“' if total_fail == 0 else f'{total_fail} TESTS FAILED âœ—'\n",
    "print(f'  {overall}')\n",
    "print('=' * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Section 8: CI/CD â€” Continuous Integration Concept\n",
    "\n",
    "### What is CI/CD?\n",
    "\n",
    "```\n",
    "Developer                GitHub                  CI Server\n",
    "    â”‚                      â”‚                          â”‚\n",
    "    â”œâ”€â”€ git push â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                          â”‚\n",
    "    â”‚                       â”œâ”€â”€ trigger pipeline â”€â”€â”€â”€â”€â”€â–ºâ”‚\n",
    "    â”‚                       â”‚                          â”‚\n",
    "    â”‚                       â”‚                          â”‚ pip install\n",
    "    â”‚                       â”‚                          â”‚ pytest class3/\n",
    "    â”‚                       â”‚                          â”‚   âœ“ 26 passed\n",
    "    â”‚                       â”‚â—„â”€â”€â”€ PASS â†’ allow merge â”€â”€â”¤\n",
    "    â”‚â—„â”€â”€ PR green â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚\n",
    "```\n",
    "\n",
    "**Key principle**: No code reaches `main` branch without passing all tests.\n",
    "\n",
    "### GitHub Actions Workflow\n",
    "\n",
    "This is the YAML file you'd commit to `.github/workflows/ml-tests.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# .github/workflows/ml-tests.yml\n",
      "# ML Quality Gate â€” runs on every push and pull request\n",
      "\n",
      "name: ML Quality Gate\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [main, develop]\n",
      "  pull_request:\n",
      "    branches: [main]\n",
      "\n",
      "jobs:\n",
      "  test:\n",
      "    runs-on: ubuntu-latest\n",
      "    timeout-minutes: 15\n",
      "\n",
      "    steps:\n",
      "      # 1. Check out code\n",
      "      - name: Checkout repository\n",
      "        uses: actions/checkout@v4\n",
      "\n",
      "      # 2. Set up Python\n",
      "      - name: Set up Python 3.10\n",
      "        uses: actions/setup-python@v4\n",
      "        with:\n",
      "          python-version: \"3.10\"\n",
      "          cache: pip\n",
      "\n",
      "      # 3. Install dependencies\n",
      "      - name: Install dependencies\n",
      "        run: pip install -r class3/requirements-class3.txt\n",
      "\n",
      "      # 4. Run API response tests\n",
      "      - name: Run API response tests\n",
      "        run: pytest class3/test_production.py::TestAPIResponse -v --tb=short\n",
      "\n",
      "      # 5. Run accuracy quality gate\n",
      "      - name: Run accuracy threshold tests\n",
      "        run: pytest class3/test_production.py::TestAccuracyThreshold -v --tb=short\n",
      "\n",
      "      # 6. Run latency SLA tests\n",
      "      - name: Run latency SLA tests\n",
      "        run: pytest class3/test_production.py::TestLatencyThreshold -v --tb=short\n",
      "\n",
      "      # 7. Generate HTML report (artifact)\n",
      "      - name: Generate test report\n",
      "        run: pytest class3/test_production.py -v --html=test-report.html --self-contained-html\n",
      "        if: always()  # Run even if tests fail\n",
      "\n",
      "      # 8. Upload report for review\n",
      "      - name: Upload test report\n",
      "        uses: actions/upload-artifact@v4\n",
      "        if: always()\n",
      "        with:\n",
      "          name: test-report\n",
      "          path: test-report.html\n",
      "          retention-days: 30\n",
      "\n",
      "\n",
      "ðŸ“‹ Key points about this CI pipeline:\n",
      "  1. Runs on EVERY push to main/develop and every pull request\n",
      "  2. Tests are split by category for clearer failure diagnosis\n",
      "  3. HTML report is uploaded even when tests fail (post-mortem analysis)\n",
      "  4. 15-minute timeout prevents runaway jobs from blocking the pipeline\n",
      "  5. Python version is pinned to 3.10 to match production environment\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Section 8: CI/CD Pipeline Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# This YAML would live at .github/workflows/ml-tests.yml in your repository.\n",
    "# It triggers automatically on every push and pull request.\n",
    "\n",
    "github_actions_yaml = '''\n",
    "# .github/workflows/ml-tests.yml\n",
    "# ML Quality Gate â€” runs on every push and pull request\n",
    "\n",
    "name: ML Quality Gate\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, develop]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 15\n",
    "\n",
    "    steps:\n",
    "      # 1. Check out code\n",
    "      - name: Checkout repository\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      # 2. Set up Python\n",
    "      - name: Set up Python 3.10\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "          cache: pip\n",
    "\n",
    "      # 3. Install dependencies\n",
    "      - name: Install dependencies\n",
    "        run: pip install -r class3/requirements-class3.txt\n",
    "\n",
    "      # 4. Run API response tests\n",
    "      - name: Run API response tests\n",
    "        run: pytest class3/test_production.py::TestAPIResponse -v --tb=short\n",
    "\n",
    "      # 5. Run accuracy quality gate\n",
    "      - name: Run accuracy threshold tests\n",
    "        run: pytest class3/test_production.py::TestAccuracyThreshold -v --tb=short\n",
    "\n",
    "      # 6. Run latency SLA tests\n",
    "      - name: Run latency SLA tests\n",
    "        run: pytest class3/test_production.py::TestLatencyThreshold -v --tb=short\n",
    "\n",
    "      # 7. Generate HTML report (artifact)\n",
    "      - name: Generate test report\n",
    "        run: pytest class3/test_production.py -v --html=test-report.html --self-contained-html\n",
    "        if: always()  # Run even if tests fail\n",
    "\n",
    "      # 8. Upload report for review\n",
    "      - name: Upload test report\n",
    "        uses: actions/upload-artifact@v4\n",
    "        if: always()\n",
    "        with:\n",
    "          name: test-report\n",
    "          path: test-report.html\n",
    "          retention-days: 30\n",
    "'''\n",
    "\n",
    "print(github_actions_yaml)\n",
    "\n",
    "print('\\nðŸ“‹ Key points about this CI pipeline:')\n",
    "print('  1. Runs on EVERY push to main/develop and every pull request')\n",
    "print('  2. Tests are split by category for clearer failure diagnosis')\n",
    "print('  3. HTML report is uploaded even when tests fail (post-mortem analysis)')\n",
    "print('  4. 15-minute timeout prevents runaway jobs from blocking the pipeline')\n",
    "print('  5. Python version is pinned to 3.10 to match production environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Automated ML Testing Pipeline\n",
      "======================================================================\n",
      "                Stage                                                Action    Trigger\n",
      "1           Code Push           Developer pushes model update or API change  Automatic\n",
      "2  Dependency Install                pip install -r requirements-class3.txt  Automatic\n",
      "3  API Response Tests             16 tests: endpoints, error handling, auth  Automatic\n",
      "4       Accuracy Gate            5 tests: accuracy â‰¥ threshold, determinism  Automatic\n",
      "5        Latency Gate                 5 tests: p99 < SLA, degradation check  Automatic\n",
      "6   Report Generation                      HTML report uploaded as artifact  Automatic\n",
      "7      Merge Decision  PR blocked if any test fails; auto-merge if all pass  Automatic\n",
      "8        Model Deploy        Trigger Docker build + deploy on merge to main  Automatic\n",
      "\n",
      "Total automated checks: 26 tests across 3 categories\n",
      "Estimated pipeline runtime: 3-5 minutes on a standard CI runner\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Automated ML Testing Pipeline Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "pipeline_stages = [\n",
    "    ('Code Push',         'Developer pushes model update or API change',         'Automatic'),\n",
    "    ('Dependency Install','pip install -r requirements-class3.txt',               'Automatic'),\n",
    "    ('API Response Tests','16 tests: endpoints, error handling, auth',            'Automatic'),\n",
    "    ('Accuracy Gate',     '5 tests: accuracy â‰¥ threshold, determinism',           'Automatic'),\n",
    "    ('Latency Gate',      '5 tests: p99 < SLA, degradation check',               'Automatic'),\n",
    "    ('Report Generation', 'HTML report uploaded as artifact',                     'Automatic'),\n",
    "    ('Merge Decision',    'PR blocked if any test fails; auto-merge if all pass', 'Automatic'),\n",
    "    ('Model Deploy',      'Trigger Docker build + deploy on merge to main',       'Automatic'),\n",
    "]\n",
    "\n",
    "df_pipeline = pd.DataFrame(pipeline_stages, columns=['Stage', 'Action', 'Trigger'])\n",
    "df_pipeline.index += 1\n",
    "\n",
    "print('\\nAutomated ML Testing Pipeline')\n",
    "print('=' * 70)\n",
    "print(df_pipeline.to_string())\n",
    "print('\\nTotal automated checks: 26 tests across 3 categories')\n",
    "print('Estimated pipeline runtime: 3-5 minutes on a standard CI runner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Test Category | Tests | Purpose |\n",
    "|---------------|-------|--------|\n",
    "| **API Response** | 16 | Correctness, error handling, auth (401/400) |\n",
    "| **Accuracy Threshold** | 5 | Model quality gate â€” block deployment if accuracy drops |\n",
    "| **Latency Threshold** | 5 | SLA gate â€” block deployment if latency exceeds limit |\n",
    "| **Total** | **26** | Comprehensive non-functional test coverage |\n",
    "\n",
    "### pytest Best Practices\n",
    "\n",
    "1. **Test one thing per function** â€” `test_health_returns_200` not `test_health`\n",
    "2. **Descriptive names** â€” name says what it tests AND what the expected behavior is\n",
    "3. **Use fixtures** â€” `@pytest.fixture` for reusable setup (test images, tokens, DB state)\n",
    "4. **Arrange-Act-Assert (AAA)** â€” setup â†’ call â†’ verify\n",
    "5. **Test edge cases** â€” empty files, corrupt images, missing fields, boundary values\n",
    "\n",
    "### CI/CD for ML Systems\n",
    "\n",
    "A mature ML CI pipeline includes:\n",
    "- **Functional tests**: Does the API work correctly?\n",
    "- **Non-functional tests**: Is it fast enough? Accurate enough? Secure?\n",
    "- **Data validation**: Is the training data within expected distribution?\n",
    "- **Model regression**: Did the new model do WORSE than the current production model?\n",
    "- **Security scan**: Does the code have known vulnerabilities? (bandit, safety)\n",
    "\n",
    "### Next Step: Lab 3\n",
    "\n",
    "Now it's your turn. Lab 3 combines everything from Parts 1â€“3 into a **Production-Grade Testing Checklist**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ai_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
