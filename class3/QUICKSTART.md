# Class 3 — Non-Functional Testing & AI Security
## Quickstart & Teaching Guide

> **Prerequisites**: Class 1 (CNN training) and Class 2 (FastAPI) should be complete.
> Students should be comfortable with Python, PyTorch, and basic HTTP concepts.

---

## Setup (5 Minutes)

```bash
cd class3
pip install -r requirements-class3.txt
```

Verify the install worked:

```bash
python -c "import jose, bcrypt, psutil, pytest; print('All good!')"
```

Start the secure API (optional — notebooks use TestClient internally):

```bash
uvicorn api_secure:app --reload --port 8001
# API docs: http://localhost:8001/docs
```

---

## Class Flow (3 × 40-Minute Blocks)

```
Part 1                 Part 2                  Part 3
Performance            Security                Automated
Benchmarking    →      Hardening       →       Testing
(40 min)               (40 min)                (40 min)
    ↓                      ↓                       ↓
How fast?              Who can call it?         Does it still work?
How efficient?         Can it be attacked?      How do we prove it?
```

Then **Lab 3** (~60 min) combines all three into a production testing checklist.

---

## Part 1: Performance Benchmarking

**File**: `Part_1_Performance_Benchmarking.ipynb`

**What students learn:**
- Why latency percentiles (p50/p95/p99) matter more than averages
- How batch size affects throughput vs latency tradeoff
- Memory profiling with `psutil`
- Dynamic quantization (FP32 → INT8) and its limits

**Key teaching moments:**

| Concept | Where in notebook | What to say |
|---------|-------------------|-------------|
| p99 vs mean latency | Section 3 | "1% of ADAS predictions still make the car wait 8 meters" |
| Throughput curve | Section 4 | "The curve flattens — that's CPU saturation" |
| Quantization | Section 5 | "Only the FC layer gets quantized here — explain why" |
| 2×2 chart | Section 6 | Generated as `benchmarking_results.png` |

**Run time**: ~5 minutes (includes 50-run latency benchmark × 6 batch sizes)

---

## Part 2: Security Hardening

**File**: `Part_2_Security_Hardening.ipynb`

**What students learn:**
- JWT tokens: creation, signing (HS256), expiry, validation
- Rate limiting with sliding window (no Redis needed)
- How adversarial attacks (FGSM) degrade model accuracy
- Difference between 401 (unauthenticated) and 429 (rate limited)

**Key teaching moments:**

| Concept | Where in notebook | What to say |
|---------|-------------------|-------------|
| JWT structure | Section 3 | "Paste the token into jwt.io — students can see the payload" |
| Rate limiter | Section 4 | "Why defaultdict(list) instead of a counter?" |
| 401 vs 403 | Section 5 test | "401 = I don't know who you are. 403 = I know, but no." |
| FGSM attack | Section 6 | "Accuracy drops from 14% to near 0% — same image, invisible noise" |
| Adversarial chart | Section 6 | Generated as `robustness_curve_security.png` |

**Reference API**: `api_secure.py` is the production-ready version of what Part 2 builds.

---

## Part 3: Automated Testing

**File**: `Part_3_Automated_Testing.ipynb`

**What students learn:**
- The ML Testing Pyramid (unit → integration → end-to-end)
- Writing `pytest`-style test classes with Arrange-Act-Assert
- Accuracy threshold as a deployment quality gate
- Latency SLA testing (p99 matters for ADAS safety)
- How a GitHub Actions CI/CD pipeline looks

**Key teaching moments:**

| Concept | Where in notebook | What to say |
|---------|-------------------|-------------|
| Test naming | Section 4 | "`test_health_returns_200` vs `test_health` — which tells you more on failure?" |
| Quality gate | Section 5 | "10% threshold now, 70% with trained weights — set it before training" |
| Tail latency | Section 6 | "p99 matters. 1 in 100 ADAS requests at 500ms = crashes" |
| CI YAML | Section 8 | "Every `git push` runs all 26 tests — show the diagram" |

**Test count**: 26 tests across 3 categories (16 API, 5 accuracy, 5 latency)

---

## Lab 3: Production Testing Checklist

**Student file**: `Lab_3_Production_Testing_Student.ipynb`
**Solution file**: `Lab_3_Production_Testing_Solution.ipynb`

**Deliverables students produce:**

| # | Deliverable | Estimated time |
|---|-------------|----------------|
| 1 | Performance benchmark DataFrame + chart | 15 min |
| 2 | JWT + rate limiter + security checklist | 20 min |
| 3 | Three test classes (13 tests total) | 15 min |
| 4 | Quantized model comparison table | 10 min |

**Submission check** — students should see:
```
lab3_benchmark.png       ← generated by cell 5
All 13 tests pass        ← shown in test runner output
6/8 security items PASS  ← the 2 failures are expected (prod-only)
```

---

## API Reference

The `api_secure.py` is the standalone reference implementation. Run it to demo the secure API live:

```bash
uvicorn api_secure:app --reload --port 8001
```

### Get a token
```bash
curl -X POST http://localhost:8001/token \
  -d "username=testuser&password=testpassword"
# → {"access_token": "<jwt>", "token_type": "bearer"}
```

### Predict with token
```bash
curl -X POST http://localhost:8001/predict \
  -H "Authorization: Bearer <token>" \
  -F "file=@image.jpg"
```

### Hit the rate limit
```bash
# Run 11 times — the 11th returns HTTP 429
for i in {1..11}; do
  curl -s -o /dev/null -w "%{http_code}\n" \
    -X POST http://localhost:8001/predict \
    -H "Authorization: Bearer <token>" \
    -F "file=@image.jpg"
done
```

### No token → 401
```bash
curl -X POST http://localhost:8001/predict -F "file=@image.jpg"
# → 401 Unauthorized
```

---

## File Overview

| File | Purpose | Run order |
|------|---------|-----------|
| `requirements-class3.txt` | Install once | Before everything |
| `api_secure.py` | Reference secure API (optional live demo) | Anytime |
| `Part_1_Performance_Benchmarking.ipynb` | Teach latency/throughput/quantization | 1st |
| `Part_2_Security_Hardening.ipynb` | Teach JWT/rate limiting/FGSM | 2nd |
| `Part_3_Automated_Testing.ipynb` | Teach pytest, CI/CD | 3rd |
| `Lab_3_Production_Testing_Student.ipynb` | Student hands-on lab | 4th (lab time) |
| `Lab_3_Production_Testing_Solution.ipynb` | Release after submission | After lab |

---

## Key Numbers to Know

| Threshold | Value | Why |
|-----------|-------|-----|
| Accuracy threshold (demo) | 10% | Random weights ≈ 14% chance-level |
| Accuracy threshold (production) | 70% | Trained ResNet-18 achieves ~83% |
| Single request SLA | 200ms | Conservative for CPU |
| p99 latency SLA | 500ms | Conservative for CPU |
| Rate limit (api_secure.py) | 10 req/min | Demo value |
| Rate limit (lab) | 3 req/min | Easier to trigger in lab |
| JWT expiry | 30 min | Standard session length |

---

## Common Student Questions

**Q: Why not use passlib?**
A: `passlib[bcrypt]` is incompatible with `bcrypt>=4.0.0` (the currently installed version). We use `bcrypt` directly — `_bcrypt.hashpw()` and `_bcrypt.checkpw()`. Same security, no dependency conflict.

**Q: Why do my tests show ~14% accuracy with random weights?**
A: Correct — 1/7 classes = 14.3% chance level. The 10% threshold is intentionally below this so tests pass. In production, set `ACCURACY_THRESHOLD = 70.0`.

**Q: The quantization only changed the model size a tiny bit. Why?**
A: `quantize_dynamic` only quantizes `nn.Linear` layers. In ResNet-18 that is only the final FC layer (512 → 7 weights). For full INT8 coverage, you need static quantization with a calibration dataset — mention this as the next step.

**Q: Can I use Redis for rate limiting in production?**
A: Yes — the `RateLimiter` class here is in-memory and single-process only. For distributed deployments, replace `defaultdict(list)` with Redis sorted sets. The interface (`is_allowed(client_id)`) stays the same.

**Q: Where is the SECRET_KEY stored in production?**
A: Never in source code. Use `SECRET_KEY = os.getenv("JWT_SECRET_KEY")` and set it as an environment variable in your container or Azure Key Vault.

---

## Troubleshooting

**`ModuleNotFoundError: No module named 'jose'`**
```bash
pip install python-jose[cryptography]
```

**`ModuleNotFoundError: No module named 'passlib'`**
The notebooks do **not** use passlib — if you see this error, make sure you're using the latest version of the notebooks (passlib was removed in favour of direct bcrypt).

**`Port 8001 already in use`**
```bash
# Windows
netstat -ano | findstr :8001
taskkill /PID <PID> /F

# macOS / Linux
lsof -ti:8001 | xargs kill
```

**Notebook kernel crashes on first cell**
Usually a PyTorch version mismatch. Verify:
```bash
python -c "import torch; print(torch.__version__)"  # should be 2.1.0
```

---

## Learning Checklist

**Part 1 — Performance**
- [ ] Understand p50/p95/p99 and why p99 matters for safety systems
- [ ] Can explain the throughput vs latency tradeoff at different batch sizes
- [ ] Knows what `quantize_dynamic` does and its limitation (Linear only)
- [ ] `benchmarking_results.png` generated successfully

**Part 2 — Security**
- [ ] Can decode a JWT token manually (use jwt.io)
- [ ] Understands the sliding-window rate limiter implementation
- [ ] Sees accuracy drop under FGSM adversarial attack
- [ ] Knows the difference between 401, 403, and 429

**Part 3 — Testing**
- [ ] All 26 tests pass in the test runner
- [ ] Understands Arrange-Act-Assert pattern
- [ ] Can explain what a quality gate blocks and why
- [ ] Has read the GitHub Actions YAML and can explain each step

**Lab 3**
- [ ] All 13 test methods pass
- [ ] `lab3_benchmark.png` generated
- [ ] Security verification shows 5/5 checks passing
- [ ] Final checklist shows at least 10/14 items PASS
