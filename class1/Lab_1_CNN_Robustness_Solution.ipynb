{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51ed0ba",
   "metadata": {},
   "source": [
    "# Lab 1: CNN Robustness Report - SOLUTION NOTEBOOK\n",
    "\n",
    "## Objective\n",
    "Build a comprehensive CNN robustness report by:\n",
    "1. Training a baseline CNN model\n",
    "2. Detecting overfitting from training curves\n",
    "3. Applying 2 regularization techniques and comparing\n",
    "4. Running adversarial attacks (FGSM)\n",
    "5. Generating a final robustness report\n",
    "\n",
    "## Complete Implementation\n",
    "This notebook contains the full solution with detailed explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b24e4e",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and paths\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "DATASET_PATH = r\"C:\\Users\\Lucifer\\python_workspace\\BITS\\AI_Quality_Engineering\\dataset\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"val\")\n",
    "TEST_PATH = os.path.join(DATASET_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(TRAIN_PATH, transform=transform)\n",
    "val_dataset = ImageFolder(VAL_PATH, transform=transform)\n",
    "test_dataset = ImageFolder(TEST_PATH, transform=transform)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Data loaders created with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8640cd",
   "metadata": {},
   "source": [
    "## Section 2: Define Models with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model (no regularization)\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)  # Train from scratch to show overfitting\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "print(\"‚úÖ BaselineModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularized Model (identical architecture, but trained with weight decay)\n",
    "class L2RegularizedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "print(\"‚úÖ L2RegularizedModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout Model (adds dropout layer before classification)\n",
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        # Replace final layer with Dropout + Linear\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),  # Randomly drops 50% of neurons\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "print(\"‚úÖ DropoutModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5384c",
   "metadata": {},
   "source": [
    "## Section 3: Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"‚úÖ train_epoch() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130df3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate on validation or test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"‚úÖ evaluate() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac82c1",
   "metadata": {},
   "source": [
    "## Section 4: Train All Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline Model (No Regularization)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training BASELINE Model (No Regularization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_model = BaselineModel(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=0.001, weight_decay=0)  # No weight decay\n",
    "\n",
    "baseline_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(baseline_model, train_loader, criterion, optimizer_baseline, device)\n",
    "    val_loss, val_acc = evaluate(baseline_model, val_loader, criterion, device)\n",
    "    \n",
    "    baseline_history['train_loss'].append(train_loss)\n",
    "    baseline_history['train_acc'].append(train_acc)\n",
    "    baseline_history['val_loss'].append(val_loss)\n",
    "    baseline_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"‚úÖ Baseline model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df859582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train L2 Regularized Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training L2 REGULARIZED Model (Weight Decay = 0.001)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "l2_model = L2RegularizedModel(num_classes).to(device)\n",
    "optimizer_l2 = optim.Adam(l2_model.parameters(), lr=0.001, weight_decay=0.001)  # L2 regularization\n",
    "\n",
    "l2_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(l2_model, train_loader, criterion, optimizer_l2, device)\n",
    "    val_loss, val_acc = evaluate(l2_model, val_loader, criterion, device)\n",
    "    \n",
    "    l2_history['train_loss'].append(train_loss)\n",
    "    l2_history['train_acc'].append(train_acc)\n",
    "    l2_history['val_loss'].append(val_loss)\n",
    "    l2_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"‚úÖ L2 Regularized model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dropout Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training DROPOUT Model (Dropout Rate = 0.5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dropout_model = DropoutModel(num_classes, dropout_rate=0.5).to(device)\n",
    "optimizer_dropout = optim.Adam(dropout_model.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "dropout_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(dropout_model, train_loader, criterion, optimizer_dropout, device)\n",
    "    val_loss, val_acc = evaluate(dropout_model, val_loader, criterion, device)\n",
    "    \n",
    "    dropout_history['train_loss'].append(train_loss)\n",
    "    dropout_history['train_acc'].append(train_acc)\n",
    "    dropout_history['val_loss'].append(val_loss)\n",
    "    dropout_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"‚úÖ Dropout model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00449543",
   "metadata": {},
   "source": [
    "## Section 5: Detect Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee479d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overfitting analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Overfitting Analysis: Baseline vs L2 vs Dropout', fontsize=16, fontweight='bold')\n",
    "\n",
    "models_data = [\n",
    "    ('Baseline\\n(No Regularization)', baseline_history, axes[0]),\n",
    "    ('L2 Regularized\\n(Weight Decay)', l2_history, axes[1]),\n",
    "    ('Dropout\\n(Dropout Rate=0.5)', dropout_history, axes[2])\n",
    "]\n",
    "\n",
    "for model_name, history, ax in models_data:\n",
    "    ax.plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2, markersize=4)\n",
    "    ax.plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2, markersize=4)\n",
    "    ax.set_xlabel('Epoch', fontsize=11)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "    ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('overfitting_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Overfitting curves plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5293091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overfitting metrics\n",
    "overfitting_analysis = {}\n",
    "\n",
    "baseline_gap = baseline_history['train_acc'][-1] - baseline_history['val_acc'][-1]\n",
    "overfitting_analysis['Baseline'] = baseline_gap\n",
    "\n",
    "l2_gap = l2_history['train_acc'][-1] - l2_history['val_acc'][-1]\n",
    "overfitting_analysis['L2 Regularized'] = l2_gap\n",
    "\n",
    "dropout_gap = dropout_history['train_acc'][-1] - dropout_history['val_acc'][-1]\n",
    "overfitting_analysis['Dropout'] = dropout_gap\n",
    "\n",
    "print(\"\\nüìä OVERFITTING ANALYSIS (Train-Val Gap):\")\n",
    "for model_name, gap in overfitting_analysis.items():\n",
    "    print(f\"{model_name:20s}: {gap:6.2f}% gap\", end=\"\")\n",
    "    if gap > 10:\n",
    "        print(\" ‚ö†Ô∏è  HIGH OVERFITTING\")\n",
    "    elif gap > 5:\n",
    "        print(\" ‚ö†Ô∏è  MODERATE OVERFITTING\")\n",
    "    else:\n",
    "        print(\" ‚úÖ GOOD GENERALIZATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa22840",
   "metadata": {},
   "source": [
    "## Section 6: Model Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd246438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size_mb(model):\n",
    "    torch.save(model.state_dict(), \"temp_model.pth\")\n",
    "    size_mb = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)\n",
    "    os.remove(\"temp_model.pth\")\n",
    "    return size_mb\n",
    "\n",
    "# Calculate model sizes\n",
    "model_complexity = {}\n",
    "\n",
    "for model_name, model in [('Baseline', baseline_model), ('L2 Regularized', l2_model), ('Dropout', dropout_model)]:\n",
    "    params = count_parameters(model)\n",
    "    size = get_model_size_mb(model)\n",
    "    model_complexity[model_name] = {'params': params, 'size': size}\n",
    "    print(f\"{model_name:20s}: {params:,} parameters | {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a12118",
   "metadata": {},
   "source": [
    "## Section 7: Robustness Testing - Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Noise Perturbation\n",
    "def add_gaussian_noise(images, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to images.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of shape (B, C, H, W)\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "        Noisy images clipped to [-1, 1] range\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(images) * noise_std  # Create random noise\n",
    "    noisy_images = images + noise  # Add noise to images\n",
    "    return torch.clamp(noisy_images, -1, 1)  # Clip to valid range\n",
    "\n",
    "print(\"‚úÖ Gaussian noise function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980696b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack\n",
    "def fgsm_attack(model, images, labels, device, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method (FGSM) Attack.\n",
    "    \n",
    "    Creates adversarial examples by moving in the gradient direction.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        images: Input images\n",
    "        labels: True labels\n",
    "        device: Device to run on\n",
    "        epsilon: Attack strength (max perturbation per pixel)\n",
    "    \n",
    "    Returns:\n",
    "        Adversarial images\n",
    "    \"\"\"\n",
    "    images.requires_grad = True  # Enable gradient tracking\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "    \n",
    "    # Compute gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient sign and create adversarial examples\n",
    "    data_grad = images.grad.data\n",
    "    sign_data_grad = data_grad.sign()  # Sign of gradient\n",
    "    \n",
    "    # Perturb in gradient direction\n",
    "    perturbed_images = images + epsilon * sign_data_grad\n",
    "    \n",
    "    # Clip to valid range\n",
    "    return torch.clamp(perturbed_images, -1, 1).detach()\n",
    "\n",
    "print(\"‚úÖ FGSM attack function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on noisy images\n",
    "def evaluate_on_noisy(model, loader, device, noise_std=0.1):\n",
    "    \"\"\"Evaluate model on Gaussian noise-perturbed images\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Testing noise (œÉ={noise_std})\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Add Gaussian noise\n",
    "            noisy_images = add_gaussian_noise(images, noise_std=noise_std)\n",
    "            # Get predictions\n",
    "            outputs = model(noisy_images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Update accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"‚úÖ Noisy evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on adversarial images\n",
    "def evaluate_on_adversarial(model, loader, device, epsilon=0.05):\n",
    "    \"\"\"Evaluate model on FGSM adversarial examples\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=f\"Testing FGSM (Œµ={epsilon})\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Generate adversarial examples\n",
    "        adv_images = fgsm_attack(model, images.clone(), labels, device, epsilon=epsilon)\n",
    "        # Evaluate on adversarial images\n",
    "        outputs = model(adv_images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Update accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"‚úÖ Adversarial evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models on clean, noisy, and adversarial data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "robustness_results = {}\n",
    "\n",
    "for model_name, model in [('Baseline', baseline_model), ('L2 Regularized', l2_model), ('Dropout', dropout_model)]:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    # Clean accuracy\n",
    "    clean_loss, clean_acc = evaluate(model, test_loader, criterion, device)\n",
    "    # Noisy accuracy (Gaussian noise with œÉ=0.1)\n",
    "    noisy_acc = evaluate_on_noisy(model, test_loader, device, noise_std=0.1)\n",
    "    # Adversarial accuracy (FGSM with Œµ=0.05)\n",
    "    adv_acc = evaluate_on_adversarial(model, test_loader, device, epsilon=0.05)\n",
    "    \n",
    "    robustness_results[model_name] = {\n",
    "        'clean': clean_acc,\n",
    "        'noisy': noisy_acc,\n",
    "        'adversarial': adv_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Clean Accuracy:               {clean_acc:.2f}%\")\n",
    "    print(f\"  Noisy Accuracy (œÉ=0.1):        {noisy_acc:.2f}%\")\n",
    "    print(f\"  Adversarial Accuracy (Œµ=0.05): {adv_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ea3a8",
   "metadata": {},
   "source": [
    "## Section 8: Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CNN ROBUSTNESS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä ACCURACY METRICS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<25} {'Clean':<15} {'Noisy':<15} {'Adversarial':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in ['Baseline', 'L2 Regularized', 'Dropout']:\n",
    "    clean = robustness_results[model_name]['clean']\n",
    "    noisy = robustness_results[model_name]['noisy']\n",
    "    adv = robustness_results[model_name]['adversarial']\n",
    "    print(f\"{model_name:<25} {clean:>6.2f}%{'':<7} {noisy:>6.2f}%{'':<7} {adv:>6.2f}%\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  INFERENCE TIME & MODEL SIZE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<25} {'Parameters':<20} {'Size (MB)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in ['Baseline', 'L2 Regularized', 'Dropout']:\n",
    "    params = model_complexity[model_name]['params']\n",
    "    size = model_complexity[model_name]['size']\n",
    "    print(f\"{model_name:<25} {params:>12,}{'':<6} {size:>6.2f}\")\n",
    "\n",
    "print(\"\\nüîó GENERALIZATION & REGULARIZATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<25} {'Train-Val Gap':<15} {'Status':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in ['Baseline', 'L2 Regularized', 'Dropout']:\n",
    "    gap = overfitting_analysis[model_name]\n",
    "    if gap > 10:\n",
    "        status = \"High Overfitting\"\n",
    "    elif gap > 5:\n",
    "        status = \"Moderate Overfitting\"\n",
    "    else:\n",
    "        status = \"Good Generalization\"\n",
    "    print(f\"{model_name:<25} {gap:>6.2f}%{'':<7} {status:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = ['Baseline', 'L2 Regularized', 'Dropout']\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "clean_accs = [robustness_results[m]['clean'] for m in model_names]\n",
    "noisy_accs = [robustness_results[m]['noisy'] for m in model_names]\n",
    "adv_accs = [robustness_results[m]['adversarial'] for m in model_names]\n",
    "\n",
    "ax.bar(x - width, clean_accs, width, label='Clean', alpha=0.8, color='#2ecc71')\n",
    "ax.bar(x, noisy_accs, width, label='Noisy (œÉ=0.1)', alpha=0.8, color='#f39c12')\n",
    "ax.bar(x + width, adv_accs, width, label='Adversarial (Œµ=0.05)', alpha=0.8, color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Robustness Comparison: Clean vs Noisy vs Adversarial', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 105])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [ax.patches[i::len(model_names)] for i in range(len(model_names))]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('robustness_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Summary visualization created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0765",
   "metadata": {},
   "source": [
    "## Key Findings & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df87f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° OBSERVATIONS & ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. OVERFITTING ANALYSIS:\")\n",
    "print(\"   - The Baseline model shows the highest train-val gap (most overfitting)\")\n",
    "print(f\"     Baseline gap: {overfitting_analysis['Baseline']:.2f}%\")\n",
    "print(f\"     L2 Regularized gap: {overfitting_analysis['L2 Regularized']:.2f}%\")\n",
    "print(f\"     Dropout gap: {overfitting_analysis['Dropout']:.2f}%\")\n",
    "print(\"   This is expected because we trained without any regularization.\")\n",
    "\n",
    "print(\"\\n2. REGULARIZATION EFFECTIVENESS:\")\n",
    "l2_effect = overfitting_analysis['Baseline'] - overfitting_analysis['L2 Regularized']\n",
    "dropout_effect = overfitting_analysis['Baseline'] - overfitting_analysis['Dropout']\n",
    "print(f\"   - L2 Regularization reduced overfitting gap by {l2_effect:.2f}%\")\n",
    "print(f\"   - Dropout reduced overfitting gap by {dropout_effect:.2f}%\")\n",
    "if dropout_effect > l2_effect:\n",
    "    print(\"   ‚Üí Dropout is more effective at controlling overfitting\")\n",
    "else:\n",
    "    print(\"   ‚Üí L2 Regularization is more effective at controlling overfitting\")\n",
    "\n",
    "print(\"\\n3. ROBUSTNESS ASSESSMENT:\")\n",
    "for model_name in model_names:\n",
    "    clean = robustness_results[model_name]['clean']\n",
    "    adv = robustness_results[model_name]['adversarial']\n",
    "    drop = clean - adv\n",
    "    print(f\"   - {model_name}: Clean {clean:.2f}% ‚Üí Adversarial {adv:.2f}% (drop: {drop:.2f}%)\")\n",
    "\n",
    "print(\"\\n4. ACCURACY-COMPLEXITY TRADEOFF:\")\n",
    "for model_name in model_names:\n",
    "    acc = robustness_results[model_name]['clean']\n",
    "    size = model_complexity[model_name]['size']\n",
    "    print(f\"   - {model_name}: {acc:.2f}% accuracy, {size:.2f} MB model size\")\n",
    "print(\"   ‚Üí Model size is identical across all three (same architecture)\")\n",
    "print(\"   ‚Üí Performance differs due to regularization, not model complexity\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS:\")\n",
    "best_model = max(model_names, key=lambda m: robustness_results[m]['clean'])\n",
    "best_robust = min(model_names, key=lambda m: robustness_results[m]['clean'] - robustness_results[m]['adversarial'])\n",
    "print(f\"   - For highest clean accuracy: {best_model}\")\n",
    "print(f\"   - For best adversarial robustness: {best_robust}\")\n",
    "print(f\"   - Deploy {best_robust} in production for better robustness\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572c36a",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive summary table\n",
    "summary_data = {\n",
    "    'Model': model_names,\n",
    "    'Clean Acc (%)': [robustness_results[m]['clean'] for m in model_names],\n",
    "    'Noisy Acc (%)': [robustness_results[m]['noisy'] for m in model_names],\n",
    "    'Adversarial Acc (%)': [robustness_results[m]['adversarial'] for m in model_names],\n",
    "    'Overfitting Gap (%)': [overfitting_analysis[m] for m in model_names],\n",
    "    'Model Size (MB)': [model_complexity[m]['size'] for m in model_names],\n",
    "    'Parameters': [model_complexity[m]['params'] for m in model_names]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìã COMPREHENSIVE SUMMARY TABLE:\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n‚úÖ Lab 1 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
