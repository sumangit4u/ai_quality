{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6964ac",
   "metadata": {},
   "source": [
    "# Class 2 - Part 2: Integration Testing & Multi-Model API\n",
    "\n",
    "## Objective\n",
    "In this session, you will:\n",
    "1. Create two model versions (baseline and dropout-regularized)\n",
    "2. Implement an API that routes requests between multiple versions\n",
    "3. Log and compare predictions from both versions\n",
    "4. Measure latency and throughput\n",
    "5. Understand canary deployments and A/B testing\n",
    "\n",
    "## Key Concepts\n",
    "- **Model Registry**: Managing multiple versions of the same model\n",
    "- **Canary Testing**: Gradually rolling out new versions to a subset of traffic\n",
    "- **A/B Testing**: Comparing two versions' performance on live traffic\n",
    "- **Latency Monitoring**: Tracking inference speed across versions\n",
    "- **Throughput**: How many requests the API can handle per second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d1be6",
   "metadata": {},
   "source": [
    "## Why Integration Testing?\n",
    "\n",
    "A single model in isolation may perform well, but production ML involves:\n",
    "- **Multiple versions**: Upgrades, rollbacks, experiments\n",
    "- **Varying input patterns**: Real-world data isn't uniform\n",
    "- **System load**: API must handle many concurrent requests\n",
    "- **Dependencies**: Integration with other services\n",
    "- **Monitoring**: Comparing versions and detecting degradation\n",
    "\n",
    "**Integration Testing** ensures the entire system works together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10ff5c",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Create Two Model Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816571e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.testclient import TestClient\n",
    "from pydantic import BaseModel\n",
    "\n",
    "print(\"âœ… Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_names = ['animal', 'name_board', 'other_vehicle', 'pedestrian', 'pothole', 'road_sign', 'speed_breaker']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2247016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architectures\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Version 1: Basic ResNet-18 without regularization\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class DropoutModel(nn.Module):\n",
    "    \"\"\"Version 2: ResNet-18 with Dropout for regularization\"\"\"\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        # Replace final layer with dropout + linear\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Create both models\n",
    "model_v1 = BaselineModel(num_classes).to(device)\n",
    "model_v2 = DropoutModel(num_classes).to(device)\n",
    "\n",
    "model_v1.eval()\n",
    "model_v2.eval()\n",
    "\n",
    "print(\"âœ… Model V1 (Baseline) created\")\n",
    "print(\"âœ… Model V2 (Dropout) created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"âœ… Transformation pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4dd079",
   "metadata": {},
   "source": [
    "## Step 2: Prediction Logging & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction log for tracking and analysis\n",
    "prediction_logs = []\n",
    "\n",
    "class PredictionLog:\n",
    "    \"\"\"Store prediction metadata for analysis\"\"\"\n",
    "    def __init__(self, timestamp, image_id, model_version, prediction, confidence, latency_ms):\n",
    "        self.timestamp = timestamp\n",
    "        self.image_id = image_id\n",
    "        self.model_version = model_version\n",
    "        self.prediction = prediction\n",
    "        self.confidence = confidence\n",
    "        self.latency_ms = latency_ms\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'timestamp': self.timestamp,\n",
    "            'image_id': self.image_id,\n",
    "            'model_version': self.model_version,\n",
    "            'prediction': self.prediction,\n",
    "            'confidence': self.confidence,\n",
    "            'latency_ms': self.latency_ms\n",
    "        }\n",
    "\n",
    "print(\"âœ… Prediction logging system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d13b28",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Model FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b394792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Schemas\n",
    "class PredictionResponse(BaseModel):\n",
    "    image_id: str\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    model_version: str\n",
    "    latency_ms: float\n",
    "\n",
    "class ComparisonResponse(BaseModel):\n",
    "    image_id: str\n",
    "    v1_prediction: str\n",
    "    v1_confidence: float\n",
    "    v2_prediction: str\n",
    "    v2_confidence: float\n",
    "    agreement: bool  # True if both models agree\n",
    "    v1_latency_ms: float\n",
    "    v2_latency_ms: float\n",
    "\n",
    "class MetricsResponse(BaseModel):\n",
    "    total_requests: int\n",
    "    v1_requests: int\n",
    "    v2_requests: int\n",
    "    avg_latency_ms: float\n",
    "    agreement_rate: float\n",
    "\n",
    "print(\"âœ… API response schemas defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"ADAS Multi-Model API\",\n",
    "    description=\"API serving two CNN model versions for A/B testing\",\n",
    "    version=\"2.0\"\n",
    ")\n",
    "\n",
    "# Traffic distribution (simulating canary deployment)\n",
    "CANARY_PERCENTAGE = 30  # Send 30% to v2, 70% to v1\n",
    "\n",
    "print(f\"âœ… FastAPI app created with {CANARY_PERCENTAGE}% canary traffic to v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3dbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/predict\")\n",
    "async def predict_cached_routing(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Route prediction request to v1 or v2 based on canary percentage.\n",
    "    \n",
    "    Returns:\n",
    "        PredictionResponse with prediction from assigned model version\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    image_id = f\"{datetime.now().timestamp()}\"\n",
    "    \n",
    "    try:\n",
    "        # Validate file\n",
    "        allowed_extensions = {'.jpg', '.jpeg', '.png', '.gif'}\n",
    "        file_ext = Path(file.filename).suffix.lower()\n",
    "        \n",
    "        if file_ext not in allowed_extensions:\n",
    "            raise HTTPException(status_code=400, detail=f\"Invalid file type: {file_ext}\")\n",
    "        \n",
    "        # Read image\n",
    "        contents = await file.read()\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=f\"Could not open image: {str(e)}\")\n",
    "        \n",
    "        # Route decision: random traffic split\n",
    "        use_v2 = random.random() < (CANARY_PERCENTAGE / 100)\n",
    "        model = model_v2 if use_v2 else model_v1\n",
    "        model_version = \"v2.0\" if use_v2 else \"v1.0\"\n",
    "        \n",
    "        # Transform and predict\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "        \n",
    "        predicted_class = class_names[predicted_idx.item()]\n",
    "        confidence_value = confidence.item()\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Log prediction\n",
    "        log = PredictionLog(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            image_id=image_id,\n",
    "            model_version=model_version,\n",
    "            prediction=predicted_class,\n",
    "            confidence=round(confidence_value, 4),\n",
    "            latency_ms=round(latency, 2)\n",
    "        )\n",
    "        prediction_logs.append(log)\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            image_id=image_id,\n",
    "            prediction=predicted_class,\n",
    "            confidence=confidence_value,\n",
    "            model_version=model_version,\n",
    "            latency_ms=round(latency, 2)\n",
    "        )\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"âœ… /predict endpoint with traffic routing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/predict-both\")\n",
    "async def predict_both_models(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Get predictions from BOTH models for comparison (A/B testing).\n",
    "    \n",
    "    Returns:\n",
    "        ComparisonResponse showing both predictions and agreement\n",
    "    \"\"\"\n",
    "    start_time_v1 = time.time()\n",
    "    image_id = f\"{datetime.now().timestamp()}\"\n",
    "    \n",
    "    try:\n",
    "        # Validate file\n",
    "        allowed_extensions = {'.jpg', '.jpeg', '.png', '.gif'}\n",
    "        file_ext = Path(file.filename).suffix.lower()\n",
    "        \n",
    "        if file_ext not in allowed_extensions:\n",
    "            raise HTTPException(status_code=400, detail=f\"Invalid file type: {file_ext}\")\n",
    "        \n",
    "        # Read image\n",
    "        contents = await file.read()\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=f\"Could not open image: {str(e)}\")\n",
    "        \n",
    "        # Transform\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Model V1 prediction\n",
    "        start_v1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs_v1 = model_v1(image_tensor)\n",
    "            probs_v1 = torch.softmax(outputs_v1, dim=1)\n",
    "            conf_v1, idx_v1 = torch.max(probs_v1, 1)\n",
    "        latency_v1 = (time.time() - start_v1) * 1000\n",
    "        \n",
    "        # Model V2 prediction\n",
    "        start_v2 = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs_v2 = model_v2(image_tensor)\n",
    "            probs_v2 = torch.softmax(outputs_v2, dim=1)\n",
    "            conf_v2, idx_v2 = torch.max(probs_v2, 1)\n",
    "        latency_v2 = (time.time() - start_v2) * 1000\n",
    "        \n",
    "        # Predictions\n",
    "        pred_v1 = class_names[idx_v1.item()]\n",
    "        pred_v2 = class_names[idx_v2.item()]\n",
    "        agreement = pred_v1 == pred_v2\n",
    "        \n",
    "        # Log both\n",
    "        log_v1 = PredictionLog(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            image_id=image_id,\n",
    "            model_version=\"v1.0\",\n",
    "            prediction=pred_v1,\n",
    "            confidence=round(conf_v1.item(), 4),\n",
    "            latency_ms=round(latency_v1, 2)\n",
    "        )\n",
    "        log_v2 = PredictionLog(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            image_id=image_id,\n",
    "            model_version=\"v2.0\",\n",
    "            prediction=pred_v2,\n",
    "            confidence=round(conf_v2.item(), 4),\n",
    "            latency_ms=round(latency_v2, 2)\n",
    "        )\n",
    "        prediction_logs.append(log_v1)\n",
    "        prediction_logs.append(log_v2)\n",
    "        \n",
    "        return ComparisonResponse(\n",
    "            image_id=image_id,\n",
    "            v1_prediction=pred_v1,\n",
    "            v1_confidence=round(conf_v1.item(), 4),\n",
    "            v2_prediction=pred_v2,\n",
    "            v2_confidence=round(conf_v2.item(), 4),\n",
    "            agreement=agreement,\n",
    "            v1_latency_ms=round(latency_v1, 2),\n",
    "            v2_latency_ms=round(latency_v2, 2)\n",
    "        )\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"âœ… /predict-both endpoint defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    \"\"\"\n",
    "    Get aggregated metrics from prediction logs.\n",
    "    \"\"\"\n",
    "    if not prediction_logs:\n",
    "        return MetricsResponse(\n",
    "            total_requests=0,\n",
    "            v1_requests=0,\n",
    "            v2_requests=0,\n",
    "            avg_latency_ms=0.0,\n",
    "            agreement_rate=0.0\n",
    "        )\n",
    "    \n",
    "    df = pd.DataFrame([log.to_dict() for log in prediction_logs])\n",
    "    \n",
    "    total_requests = len(df)\n",
    "    v1_requests = len(df[df['model_version'] == 'v1.0'])\n",
    "    v2_requests = len(df[df['model_version'] == 'v2.0'])\n",
    "    avg_latency = df['latency_ms'].mean()\n",
    "    \n",
    "    # Agreement rate: compare predictions with same image_id\n",
    "    agreement_count = 0\n",
    "    comparison_pairs = 0\n",
    "    \n",
    "    for image_id in df['image_id'].unique():\n",
    "        logs_for_image = df[df['image_id'] == image_id]\n",
    "        if len(logs_for_image) == 2:  # Both v1 and v2 predictions\n",
    "            v1_logs = logs_for_image[logs_for_image['model_version'] == 'v1.0']\n",
    "            v2_logs = logs_for_image[logs_for_image['model_version'] == 'v2.0']\n",
    "            if not v1_logs.empty and not v2_logs.empty:\n",
    "                if v1_logs.iloc[0]['prediction'] == v2_logs.iloc[0]['prediction']:\n",
    "                    agreement_count += 1\n",
    "                comparison_pairs += 1\n",
    "    \n",
    "    agreement_rate = (agreement_count / comparison_pairs * 100) if comparison_pairs > 0 else 0\n",
    "    \n",
    "    return MetricsResponse(\n",
    "        total_requests=total_requests,\n",
    "        v1_requests=v1_requests,\n",
    "        v2_requests=v2_requests,\n",
    "        avg_latency_ms=round(avg_latency, 2),\n",
    "        agreement_rate=round(agreement_rate, 2)\n",
    "    )\n",
    "\n",
    "print(\"âœ… /metrics endpoint defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53809501",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/logs\")\n",
    "async def get_logs():\n",
    "    \"\"\"\n",
    "    Get all prediction logs (for detailed analysis).\n",
    "    \"\"\"\n",
    "    logs_dict = [log.to_dict() for log in prediction_logs]\n",
    "    return {\"total_logs\": len(logs_dict), \"logs\": logs_dict}\n",
    "\n",
    "print(\"âœ… /logs endpoint defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72a73",
   "metadata": {},
   "source": [
    "## Step 4: Test the Multi-Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test client\n",
    "client = TestClient(app)\n",
    "print(\"âœ… Test client created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create test images\n",
    "def create_test_image(color='red'):\n",
    "    \"\"\"Create a dummy image for testing\"\"\"\n",
    "    image = Image.new('RGB', (100, 100), color=color)\n",
    "    img_bytes = io.BytesIO()\n",
    "    image.save(img_bytes, format='PNG')\n",
    "    img_bytes.seek(0)\n",
    "    return img_bytes\n",
    "\n",
    "print(\"âœ… Test image helper created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013744c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Canary Routing (requests sent to v1 or v2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: Canary Deployment Routing (30% to v2, 70% to v1)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(10):\n",
    "    img = create_test_image(color='red')\n",
    "    response = client.post(\"/predict\", files={\"file\": (\"test.png\", img, \"image/png\")})\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Request {i+1}: Model={result['model_version']}, Latency={result['latency_ms']:.2f}ms\")\n",
    "\n",
    "print(\"\\nâœ… Traffic successfully routed between versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab01150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: A/B Testing - Get predictions from both models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: A/B Testing - Compare Both Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(5):\n",
    "    img = create_test_image(color='blue')\n",
    "    response = client.post(\"/predict-both\", files={\"file\": (\"test.png\", img, \"image/png\")})\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        agreement_icon = \"âœ…\" if result['agreement'] else \"âŒ\"\n",
    "        print(f\"\\nComparison {i+1}:\")\n",
    "        print(f\"  V1: {result['v1_prediction']} (conf={result['v1_confidence']:.4f}, latency={result['v1_latency_ms']:.2f}ms)\")\n",
    "        print(f\"  V2: {result['v2_prediction']} (conf={result['v2_confidence']:.4f}, latency={result['v2_latency_ms']:.2f}ms)\")\n",
    "        print(f\"  Agreement: {agreement_icon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de15483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: View Metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: Aggregated Metrics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response = client.get(\"/metrics\")\n",
    "if response.status_code == 200:\n",
    "    metrics = response.json()\n",
    "    print(f\"Total Requests: {metrics['total_requests']}\")\n",
    "    print(f\"V1 Requests: {metrics['v1_requests']} ({metrics['v1_requests']/metrics['total_requests']*100:.1f}%)\")\n",
    "    print(f\"V2 Requests: {metrics['v2_requests']} ({metrics['v2_requests']/metrics['total_requests']*100:.1f}%)\")\n",
    "    print(f\"Average Latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "    print(f\"Model Agreement Rate: {metrics['agreement_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7905b",
   "metadata": {},
   "source": [
    "## Step 5: Performance Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get all logs\n",
    "response = client.get(\"/logs\")\n",
    "logs_data = response.json()['logs']\n",
    "df = pd.DataFrame(logs_data)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Version comparison\n",
    "    print(\"\\nğŸ“Š Latency by Model Version:\")\n",
    "    version_stats = df.groupby('model_version')['latency_ms'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "    print(version_stats.round(2))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Latency distribution\n",
    "    for version in df['model_version'].unique():\n",
    "        version_latencies = df[df['model_version'] == version]['latency_ms']\n",
    "        axes[0].hist(version_latencies, alpha=0.6, label=version, bins=10)\n",
    "    axes[0].set_xlabel('Latency (ms)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Inference Latency Distribution by Version')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Traffic distribution\n",
    "    version_counts = df['model_version'].value_counts()\n",
    "    axes[1].bar(version_counts.index, version_counts.values, color=['blue', 'orange'])\n",
    "    axes[1].set_ylabel('Number of Requests')\n",
    "    axes[1].set_title('Traffic Distribution (Canary Test)')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(version_counts.values):\n",
    "        percentage = v / version_counts.sum() * 100\n",
    "        axes[1].text(i, v + 0.1, f'{percentage:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('integration_testing_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Analysis visualization saved\")\n",
    "else:\n",
    "    print(\"No logs to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8c600",
   "metadata": {},
   "source": [
    "## Step 6: Key Findings - Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ INTEGRATION TESTING INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    v1_latency = df[df['model_version'] == 'v1.0']['latency_ms'].mean()\n",
    "    v2_latency = df[df['model_version'] == 'v2.0']['latency_ms'].mean()\n",
    "    \n",
    "    print(f\"\"\"\n",
    "1ï¸âƒ£ CANARY DEPLOYMENT:\n",
    "   - Safely test v2 with {CANARY_PERCENTAGE}% of traffic\n",
    "   - Main traffic continues with stable v1\n",
    "   - If v2 shows issues, rollback unaffects most users\n",
    "   \n",
    "2ï¸âƒ£ LATENCY COMPARISON:\n",
    "   - V1 Average Latency: {v1_latency:.2f}ms\n",
    "   - V2 Average Latency: {v2_latency:.2f}ms\n",
    "   - Difference: {abs(v1_latency - v2_latency):.2f}ms\n",
    "   \n",
    "3ï¸âƒ£ MODEL AGREEMENT:\n",
    "   - High agreement rate = Both versions are stable\n",
    "   - Low agreement = Investigate model divergence\n",
    "   \n",
    "4ï¸âƒ£ THROUGHPUT:\n",
    "   - Total Requests Processed: {len(df)}\n",
    "   - Requests per second: ~{len(df)/5:.1f} req/sec (demo rate)\n",
    "   \n",
    "5ï¸âƒ£ MONITORING ALERTS:\n",
    "   âœ… Track latency increase > 20% = Performance degradation\n",
    "   âœ… Agreement < 70% = Model quality issue\n",
    "   âœ… Error rate > 5% = API stability problem\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6c344",
   "metadata": {},
   "source": [
    "## Step 7: Deployment Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ğŸš€ PRODUCTION DEPLOYMENT STRATEGIES\n",
    "\n",
    "1. BLUE-GREEN DEPLOYMENT:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  Blue (v1.0)    â”‚  Green (v2.0)    â”‚\n",
    "   â”‚  Production     â”‚  Staging         â”‚\n",
    "   â”‚  100% traffic   â”‚  Testing         â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†’ When v2 is ready, switch ALL traffic instantly\n",
    "   â†’ If issues, switch back immediately\n",
    "   â†’ Zero downtime, instant rollback\n",
    "\n",
    "2. CANARY DEPLOYMENT (Current Approach):\n",
    "   10% â†’ 25% â†’ 50% â†’ 100%\n",
    "   â†’ Gradually increase v2 traffic\n",
    "   â†’ Monitor metrics at each stage\n",
    "   â†’ Stop if degradation detected\n",
    "\n",
    "3. SHADOW DEPLOYMENT:\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  User Request               â”‚\n",
    "   â”‚         â†“                   â”‚\n",
    "   â”‚  V1 (Primary)    V2 (Shadow)â”‚\n",
    "   â”‚  Returns to user  No return â”‚\n",
    "   â”‚                   Compare   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†’ v2 processes in background\n",
    "   â†’ Compare predictions offline\n",
    "   â†’ No user impact\n",
    "\n",
    "4. A/B TESTING:\n",
    "   â†’ Different versions to different user cohorts\n",
    "   â†’ Statistical significance testing\n",
    "   â†’ Winner deployed to all\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de808a",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ğŸ“ KEY TAKEAWAYS FROM PART 2\n",
    "\n",
    "âœ… APIs must handle multiple model versions simultaneously\n",
    "âœ… Canary deployments reduce risk of bad model releases  \n",
    "âœ… A/B testing provides data-driven model selection\n",
    "âœ… Latency monitoring catches performance regressions\n",
    "âœ… Agreement rates indicate model stability\n",
    "âœ… Comprehensive logging enables root cause analysis\n",
    "âœ… Gradual rollouts are safer than big bangs\n",
    "\n",
    "ğŸ”‘ CRITICAL METRICS TO TRACK:\n",
    "   1. Latency per model version\n",
    "   2. Error rates and types\n",
    "   3. Prediction agreement rates\n",
    "   4. Throughput capacity\n",
    "   5. Confidence distribution\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
